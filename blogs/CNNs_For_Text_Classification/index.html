<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>CNNs for Text Classification - Paddy - Paddy&#039;s Log Book</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Paddy - Paddy&#039;s Log Book"><meta name="msapplication-TileImage" content="/images/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Paddy - Paddy&#039;s Log Book"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="CNNs for Text Classification"><meta property="og:type" content="blog"><meta property="og:title" content="CNNs for Text Classification"><meta property="og:url" content="http://paddyzz.github.io/blogs/CNNs_For_Text_Classification/"><meta property="og:site_name" content="Paddy - Paddy&#039;s Log Book"><meta property="og:description" content="CNNs for Text Classification"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://paddyzz.github.io/images/assets/cnns_for_text_classifications/conv_maxpooling_steps.gif"><meta property="og:image" content="http://paddyzz.github.io/images/assets/cnns_for_text_classifications/token_dictionary.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/cnns_for_text_classifications/onehot.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/cnns_for_text_classifications/word_embeddings.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/cnns_for_text_classifications/context_drink.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/cnns_for_text_classifications/conv_dimensions.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/cnns_for_text_classifications/conv_kernel_operation.gif"><meta property="og:image" content="http://paddyzz.github.io/images/assets/cnns_for_text_classifications/similar_phrases_conv_out.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/cnns_for_text_classifications/conv_1D_time.gif"><meta property="og:image" content="http://paddyzz.github.io/images/assets/cnns_for_text_classifications/maxpooling_over_time.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/cnns_for_text_classifications/softmax.png"><meta property="article:published_time" content="2024-10-15T14:26:41.183Z"><meta property="article:modified_time" content="2024-10-23T16:00:00.000Z"><meta property="article:author" content="Paddy"><meta property="article:tag" content="DL"><meta property="article:tag" content="NLP"><meta property="article:tag" content="ML"><meta property="article:tag" content="CNN"><meta property="article:tag" content="Theory"><meta property="article:tag" content="NoCode"><meta property="article:tag" content="Word2Vec"><meta property="article:tag" content="Word Embedding"><meta property="article:tag" content="Convolution Layers"><meta property="article:tag" content="Max Pooling"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://paddyzz.github.io/images/assets/cnns_for_text_classifications/conv_maxpooling_steps.gif"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://paddyzz.github.io/blogs/CNNs_For_Text_Classification/"},"headline":"CNNs for Text Classification","image":["http://paddyzz.github.io/images/assets/cnns_for_text_classifications/conv_maxpooling_steps.gif","http://paddyzz.github.io/images/assets/cnns_for_text_classifications/token_dictionary.png","http://paddyzz.github.io/images/assets/cnns_for_text_classifications/onehot.png","http://paddyzz.github.io/images/assets/cnns_for_text_classifications/word_embeddings.png","http://paddyzz.github.io/images/assets/cnns_for_text_classifications/context_drink.png","http://paddyzz.github.io/images/assets/cnns_for_text_classifications/conv_dimensions.png","http://paddyzz.github.io/images/assets/cnns_for_text_classifications/conv_kernel_operation.gif","http://paddyzz.github.io/images/assets/cnns_for_text_classifications/similar_phrases_conv_out.png","http://paddyzz.github.io/images/assets/cnns_for_text_classifications/conv_1D_time.gif","http://paddyzz.github.io/images/assets/cnns_for_text_classifications/maxpooling_over_time.png","http://paddyzz.github.io/images/assets/cnns_for_text_classifications/softmax.png"],"datePublished":"2024-10-15T14:26:41.183Z","dateModified":"2024-10-23T16:00:00.000Z","author":{"@type":"Person","name":"Paddy"},"publisher":{"@type":"Organization","name":"Paddy - Paddy's Log Book","logo":{"@type":"ImageObject","url":"http://paddyzz.github.io/images/favicon.png"}},"description":"CNNs for Text Classification"}</script><link rel="canonical" href="http://paddyzz.github.io/blogs/CNNs_For_Text_Classification/"><link rel="icon" href="/images/favicon.png"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.2/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><meta name="follow.it-verification-code" content="SdbKP9l6XayZRBYCuvDS"><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-3-column"><script type="text/javascript" src="/js/svg-inject.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/gsap@3.3.4/dist/gsap.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/gsap@3.3.4/dist/ScrollTrigger.js"></script><progress max="100" value="0"></progress><canvas id="universe"></canvas><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/images/favicon.png" alt="Paddy - Paddy&#039;s Log Book" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item navbar-item-home" href="/">PADDY&#039;S LOG BOOK</a><a class="navbar-item" href="/curriculum/">CURRICULUM</a><a class="navbar-item" href="/Certificates/">CERTIFICATES</a><a class="navbar-item" href="/blogs/">BLOGS</a><a class="navbar-item" href="/projects/">PROJECTS</a><a class="navbar-item" href="/life/">LIFE</a><a class="navbar-item" href="/archives/">ARCHIVES</a><a class="navbar-item" href="/categories/">CATEGORIES</a><a class="navbar-item" href="/tags/">TAGS</a><a class="navbar-item" href="/faqs/">FAQS</a></div><div class="navbar-end"><a class="navbar-item navbar-item-logo" id="star-nav" title="Nox Tenebratio!" style="opacity:0;display:none;userSelect:none;" href="javascript:;"><i class="fa fa-star-and-crescent" id="star-icon" style="font-size:17.75px"></i></a><a class="navbar-item navbar-item-logo night" id="night-nav" title="Nox!" href="javascript:;"><i class="fas fa-moon" id="night-icon" style="font-size:16.75px"></i></a><a class="navbar-item navbar-item-logo" id="night-nav" title="Email" href="mailto:jiahe.zhao@uq.net.au?body=Hello%2C%0A%0AIt%20would%20be%20greatly%20appreciated%20if%20you%20could%20send%20me%20an%20email%20using%20your%20personal%20email%20client%20rather%20than%20the%20current%20HTML%20popup%20one%2C%20as%20I%20may%20not%20receive%20your%20message.%0A%0AThank%20you."><i class="fas fa-envelope" style="font-size:16.75px"></i></a><a class="navbar-item navbar-item-logo" title="Gitter" target="_blank" rel="noopener" href="https://app.gitter.im/#/room/#Paddy/Community:gitter.im"><i class="fab fa-gitter" style="font-size:16.75px"></i></a><a class="navbar-item navbar-item-logo" title="Element" target="_blank" rel="noopener" href="https://app.element.io/#/room/#Paddy/Community:gitter.im"><svg version="1.0" id="svg-element" xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 225 225"><g fill="#4a4a4a"><path d="M91 1c-11 3-13 19-3 24l11 2c32 3 56 27 59 59 0 9 2 12 6 15 7 4 15 2 19-5l1-13c-1-11-3-19-9-31A87 87 0 0 0 91 1zM73 43a90 90 0 0 0-72 91c3 11 19 13 24 3l2-11c3-32 27-56 59-59 9 0 12-2 15-6 4-7 2-15-5-19H73zM208 82c-7 2-9 6-10 17-3 32-27 56-59 59-9 0-12 2-15 6-4 7-1 15 5 19l13 1c12-1 20-3 33-9a86 86 0 0 0 49-84c-1-4-6-9-8-9h-8zM47 124c-5 3-6 7-6 18 1 12 3 20 10 33 13 28 39 46 71 49 10 1 14 0 18-3 6-6 4-17-3-21l-11-2c-32-3-56-27-59-59l-2-10c-2-4-7-7-11-7l-7 2z"></path></g></svg></a><a class="navbar-item navbar-item-logo" title="GitHub" target="_blank" rel="noopener" href="https://github.com/paddyzz"><i class="fab fa-github" style="font-size:16.75px"></i></a><a class="navbar-item navbar-item-logo" title="Linkedin" target="_blank" rel="noopener" href="https://www.linkedin.com/in/jiahe-paddy-zhao-213b24300"><i class="fab fa-linkedin" style="font-size:19.75px"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item navbar-item-logo search" title="Search For It !" href="javascript:;"><i class="fa-brands fa-searchengin" style="font-size:19.75px"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-3 is-size-4-mobile title-font-style">CNNs for Text Classification</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item author-capitalize"> Paddy </span><span class="level-item"><i class="far fa-calendar-alt"></i>&nbsp;<time dateTime="2024-10-15T14:26:41.183Z" title="15/10/2024, 10:26:41 pm">15-10-2024</time></span><span class="level-item"><i class="far fa-calendar-check"></i>&nbsp;<time dateTime="2024-10-23T16:00:00.000Z" title="24/10/2024, 12:00:00 am">24-10-2024</time></span><span class="level-item"><i class="far fa-folder-open has-text-grey"></i><span> </span><a class="link-muted" href="/blogs/">blogs</a></span><span class="level-item"> <i class="far fa-clock"></i> <span> </span> 15 minutes read<span> </span>( About 2186 words)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv">0</span>&nbsp;visits</span></div></div><article class="message is-info" style="margin-top:1.5rem;font-style:oblique"><div class="message-body">CNNs for Text Classification</div></article><div class="content card-content-font-style" style="margin-top:1.5rem;margin-bottom:1rem"><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>This article will discuss how to use convolutional neural networks to identify general patterns in text and perform text classification.</p>
<p>Firstly, how do we represent text and prepare it as input for neural networks? Let’s focus on the case of classifying movie reviews, although the same techniques can be applied to articles, tweets, search queries, and more. For movie reviews, we have hundreds of written comments, each consisting of long, opinionated sentences that vary in length.</p>
<p><img src="/images/assets/cnns_for_text_classifications/conv_maxpooling_steps.gif"></p>
<ol>
<li><p>Neural networks can only learn to find patterns in numerical data. Therefore, before we feed the reviews into the neural network, we must convert each word into a number. This process is commonly referred to as <strong>word encoding</strong> or tokenization. A typical encoding process works as follows:</p>
<p> For all text data (in this case, movie reviews), we record each unique word that appears in the dataset and list them as the model’s <strong>vocabulary</strong>.</p>
</li>
<li><p>We assign a unique integer, called a token, to each word in the vocabulary. Typically, these tokens are assigned based on the frequency of the words in the dataset. Thus, the most frequently occurring word across the dataset will have an associated token: 0. For example, if the most common word is “the,” its associated token value would be 0. The next most common word would be assigned the token 1, and this process continues.</p>
</li>
<li><p>In code, this word-token association is represented in a <strong>dictionary</strong>, which maps each unique word to its token integer value.</p>
</li>
</ol>
<p><code>&#123;&#39;the&#39;: 0, &#39;of&#39;: 1, &#39;so&#39;: 2, &#39;then&#39;: 3, &#39;you&#39;: 4, … &#125;</code> </p>
<ol start="4">
<li>Finally, once these tokens are assigned to individual words, we can tokenize the entire corpus.</li>
</ol>
<p>For any document in the dataset, such as a single movie review, we treat it as a list of words in a sequence. We then use the token dictionary to convert this list of words into a list of integer values.</p>
<figcaption align="center">
  <img src="../../images/assets/cnns_for_text_classifications/token_dictionary.png" >
</figcaption>

<p>It is worth noting that these token values don’t carry much traditional meaning. In other words, we generally consider that the value 1 is closer to 2 than to 1000. As another example, we might view the value 10 as an average of 2 and 18. However, a word with a token of 1 is not necessarily more similar to a word with a token of 2 than to one with a token of 1000. The typical concept of numerical distance doesn’t provide us with any information about the relationships between individual words.</p>
<p>Therefore, we must take another encoding step; ideally, we want to either eliminate the numerical order of these tokens or represent the relationships between words.</p>
<h2 id="One-Hot-Encoding"><a href="#One-Hot-Encoding" class="headerlink" title="One-Hot Encoding"></a>One-Hot Encoding</h2><p>A common encoding step is to perform <strong>one-hot encoding</strong> for each token, representing each word as a vector with as many values as there are words in the vocabulary. That is, each column in the vector represents a possible word in the vocabulary. All positions in the vector are filled with 0s except for the index corresponding to the token value of the word (for example, the index for <code>the</code> would be 0).</p>
<figcaption align="center">
  <img src="../../images/assets/cnns_for_text_classifications/onehot.png" >
</figcaption>


<p>For larger vocabularies, these vectors can become quite long and sparse, containing 0s in all positions except one. This is considered a very <strong>sparse</strong> representation.</p>
<h2 id="Word2Vec-Word-Embeddings"><a href="#Word2Vec-Word-Embeddings" class="headerlink" title="Word2Vec Word Embeddings"></a>Word2Vec Word Embeddings</h2><p>Often, we desire a more <strong>dense</strong> representation. One such representation is learned word vectors, known as embeddings. Word embeddings are fixed-length vectors, typically around 100 dimensions, with each vector containing approximately 100 values that represent a word. The values in each column represent features of a word rather than any specific word.</p>
<figcaption align="center">
  <img src="../../images/assets/cnns_for_text_classifications/word_embeddings.png" >
</figcaption>



<p>These embeddings are formed through training a single-layer neural network (<strong>Word2Vec</strong> model) in an unsupervised manner on the input words and some surrounding words in sentences.</p>
<figcaption align="center">
  <img src="../../images/assets/cnns_for_text_classifications/context_drink.png" >
</figcaption>

<p>Words that appear in similar contexts (and have similar surrounding words), such as <code>coffee</code>, <code>tea</code>, and <code>water</code>, tend to have similar vectors; the vectors point in roughly the same direction. Thus, similar words can be found by calculating the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a> between word vectors. And there are useful resources available for references, including <a target="_blank" rel="noopener" href="https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/Skip_Grams_Solution.ipynb">this illustrative code example</a>.</p>
<p>These word embeddings possess some very favorable properties:</p>
<ul>
<li>The embeddings are dense vector representations of words.</li>
<li>Words with similar contexts often have embeddings that point in the same direction.</li>
<li>These word embeddings can serve as inputs to recurrent neural networks or convolutional neural networks.</li>
</ul>
<h2 id="Convolutional-Kernel"><a href="#Convolutional-Kernel" class="headerlink" title="Convolutional Kernel"></a>Convolutional Kernel</h2><p>The convolutional layer aims to detect spatial patterns in images by sliding a small kernel window across the image. These windows are typically small, with sizes like 3x3 pixels, and each kernel unit has an associated weight. As the kernel slides pixel-by-pixel over the image, the kernel weights are multiplied by the pixel values in the underlying image, and the sum of all these products yields an output, the filtered pixel value.</p>
<p>In the case of text classification, the convolutional kernel still acts as a sliding window, but its job is to examine multiple word embeddings rather than small areas of pixels in an image. Depending on the task, the size of the convolutional kernel must also change. To look at a sequence of word embeddings, we need a window that examines multiple word embeddings in the sequence. The kernel will no longer be square but will be a wide rectangle of sizes like 3x300 or 5x300 (assuming the embedding length is 300). </p>
<p>The height of the kernel corresponds to the number of embeddings it sees at once, similar to representing n-grams in word models. </p>
<p>The width of the kernel should span the entire length of the word embeddings. </p>
<figcaption align="center">
  <img src="../../images/assets/cnns_for_text_classifications/conv_dimensions.png" >
</figcaption>

<h2 id="Convolution-on-Word-Sequences"><a href="#Convolution-on-Word-Sequences" class="headerlink" title="Convolution on Word Sequences"></a>Convolution on Word Sequences</h2><p>Let’s take a look at an example of filtered word embeddings for a pair of (2-grams). Below, we have a toy example showing each word encoded as an embedding with 3 values; usually, this would contain more values, but this is for visualization purposes. </p>
<p>To observe the two words in this example sequence, we can use a 2x3 convolutional kernel. The kernel weights are placed on top of the two word embeddings; in this case, the downward direction represents time, so within this short sequence, the word “movie” follows closely after the word “good.” The kernel weights and embedding values are multiplied together, and then summed to obtain a <strong>single output value</strong> of 0.54. </p>
<p><img src="/images/assets/cnns_for_text_classifications/conv_kernel_operation.gif"></p>
<p>The convolutional neural network will contain many such kernels, and during network training, these kernel weights are learned. Each kernel is designed to view a word along with its surrounding words in a sequential window and produce a value that captures information about that phrase. Thus, the convolution operation can be seen as <strong>window-based feature extraction</strong>, where features are patterns in groups of sequential words indicating sentiment, grammatical function, etc. </p>
<p>It’s also worth noting that the input channel count for this convolutional layer (typically 1 for grayscale images and 3 for RGB images) in this example will be 1 since a single input text source will be encoded as a list of word embeddings. </p>
<h2 id="Recognizing-General-Patterns"><a href="#Recognizing-General-Patterns" class="headerlink" title="Recognizing General Patterns"></a>Recognizing General Patterns</h2><p>This convolution operation has another beneficial characteristic. Recall that similar words will have similar embeddings, and convolution operations are merely linear operations on these vectors. Therefore, when the convolutional kernel is applied to different sets of similar words, it will yield similar output values. </p>
<figcaption align="center">
  <img src="../../images/assets/cnns_for_text_classifications/similar_phrases_conv_out.png" >
</figcaption>


<p>In the example below, we can see that the input 2-grams “good movie” and “great song” produce approximately the same convolution output values because the word embeddings for these pairs are also very similar. </p>
<p>In this case, the convolutional kernel has learned to capture more general features; not just a good movie or a great song, but overall something positive. Recognizing these high-level features is particularly useful in text classification tasks, as they often rely on general groupings. For instance, in sentiment analysis, the model benefits from being able to represent phrases that are negative, neutral, and positive. The model can use these general features to classify the entire text. </p>
<h2 id="1D-Convolution"><a href="#1D-Convolution" class="headerlink" title="1D Convolution"></a>1D Convolution</h2><p>To process entire sequences of words, these kernels will slide down the list of word embeddings sequentially. This is referred to as <strong>1D convolution</strong> because the kernels only move in one dimension: time. A single kernel moves down the input embedding list one by one, looking at the first word embedding (along with a small window of the next word embeddings), then the next, and so on. The final output will be a feature vector containing as many values as the input embeddings, so the size of the input sequence is indeed important. I say “about” because sometimes the convolutional kernels won’t perfectly cover the word embeddings, hence padding may need to be included to account for the kernel’s height. The previous <a target="_blank" rel="noopener" href="https://cezannec.github.io/Convolutional_Neural_Networks/">article</a> describes the relationship between padding and kernel size in more detail. Below, we can see what the output of 1D convolution might look like when applied to a short sequence of word embeddings. </p>
<p><img src="/images/assets/cnns_for_text_classifications/conv_1D_time.gif"></p>
<h2 id="Multiple-Kernels"><a href="#Multiple-Kernels" class="headerlink" title="Multiple Kernels"></a>Multiple Kernels</h2><p>Just as in a typical convolutional neural network, a single convolutional kernel is insufficient to detect all the different types of features useful for classification tasks. To build a network capable of learning various relationships between words, we need many different filters of varying heights. In the paper <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1408.5882">Convolutional Neural Networks for Sentence Classification</a> (Yoon Kim, 2014), they used a total of 300 kernels; 100 kernels for each height: 3, 4, and 5. These heights effectively capture patterns in sequences of 3, 4, and 5 continuous words. They chose 5 words as a cutoff because words that are further apart generally have lower relevance or usefulness for identifying patterns within a phrase. For example, if I say <code>this apple is red, and a banana is yellow</code> then words like <code>this</code> <code>apple</code> and <code>red</code> that are close together are more relevant than words like <code>apple</code> and <code>yellow</code> that are farther apart. Therefore, as a short convolutional kernel slides over the word embeddings one at a time, it is designed to capture local features or characteristics within nearby continuous word windows. The stacked output feature vectors produced by several convolution operations are referred to as the convolutional layer. </p>
<h2 id="Maxpooling-Over-Time"><a href="#Maxpooling-Over-Time" class="headerlink" title="Maxpooling Over Time"></a>Maxpooling Over Time</h2><p>Now that we understand how the convolution operation generates feature vectors that can represent local features within a sequence of word embeddings, one consideration is what these feature vectors will look like when applied to important phrases in the text source. If we are trying to classify movie reviews and see the phrase <code>great plot</code> the position it appears in the review doesn’t matter; what matters is that it appears in the review. This serves as a strong indicator that this is a positive review, regardless of its position in the source text. </p>
<p>To indicate the presence of these high-level features, we need a method to recognize them in the vector, regardless of their location in the larger input sequence. One way to identify significant features is to discard less relevant positional information, regardless of where they are in the sequence. For this, we can use the <strong>maxpooling</strong> operation, which forces the network to retain only the maximum value from the feature vector, which should be the most useful local feature. </p>
<p>Since this operation looks at a series of local feature values, it is commonly referred to as <strong>max pooling over time</strong>. </p>
<p>By processing the maximum value produced from each convolutional feature vector, these maximums will be concatenated and passed to the final fully connected layer, which can generate as many class scores as needed for the text classification task. </p>
<figcaption align="center">
  <img src="../../images/assets/cnns_for_text_classifications/maxpooling_over_time.png" >
</figcaption>

<h2 id="Summarization"><a href="#Summarization" class="headerlink" title="Summarization"></a>Summarization</h2><p>In the text classification process using Convolutional Neural Networks (CNNs), the complete network begins by receiving a batch of texts (for example movie reviews) as input. Initially, these reviews are transformed into word embeddings through a pre-trained embedding layer, which captures semantic similarities between words in a continuous vector space. Following this, the sequences of word embeddings are processed through multiple convolutional layers with varying kernel heights of 3, 4, and 5 or even higher</p>
<p>Each convolutional layer applies filters that slide over the embedded sequences, detecting local patterns and features within the text. After the convolution operations, a ReLU activation function is applied to introduce non-linearity, enabling the model to learn complex representations. Subsequently, a max pooling operation is performed, where the maximum values from the feature maps are extracted, effectively summarizing the most salient features detected by each convolutional layer.</p>
<figcaption align="center">
  <img src="../../images/assets/cnns_for_text_classifications/softmax.png" >
</figcaption>


<p>Finally, the maximum values from the convolutional layers are concatenated to form a comprehensive feature vector. This vector is then fed into a fully connected classification layer, which outputs class scores corresponding to the  category of the texts. Through this systematic approach, CNNs leverage their ability to capture spatial hierarchies in data, making them particularly effective for text classification tasks.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a target="_blank" rel="noopener" href="https://medium.com/voice-tech-podcast/text-classification-using-cnn-9ade8155dfb9">CNNs for text classification</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1408.5882">Convolutional Neural Networks for Sentence Classification</a></p>
<p><a target="_blank" rel="noopener" href="https://cezannec.github.io/CNN_Text_Classification/">CNNs for text classfication</a></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>CNNs for Text Classification</p><p><a class="link-muted" href="http://paddyzz.github.io/blogs/CNNs_For_Text_Classification/">http://paddyzz.github.io/blogs/CNNs_For_Text_Classification/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Paddy</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>15-10-2024</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>24-10-2024</p></div></div><div class="level-item is-narrow"><div><h6>Categories</h6><a class="link-muted is-uppercase is-size-7 article_license_category" href="/blogs/">blogs</a></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons article_license_logo" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons article_license_logo" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons article_license_logo" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1.5px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 mb-4" style="line-height:21px;height:0"><i class="fas fa-tags has-text-grey"></i><span> </span><a class="link-muted  tag-uppercase" rel="tag" href="/tags/DL/">DL<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/NLP/">NLP<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/ML/">ML<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/CNN/">CNN<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/Theory/">Theory<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/NoCode/">NoCode<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/Word2Vec/">Word2Vec<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/Word-Embedding/">Word Embedding<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/Convolution-Layers/">Convolution Layers<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/Max-Pooling/">Max Pooling</a></div></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=665ef533f75dab0019ade953&amp;&amp;product=inline-share-buttons" defer></script></article></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/certificates/1/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">MOOC Certificates</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/projects/Config_Kubeflow/"><span class="level-item">Configure and deploy models on Kubeflow</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://paddyzz.github.io/blogs/CNNs_For_Text_Classification/';
            this.page.identifier = '/blogs/CNNs_For_Text_Classification/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'paddys-disqus' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div> </div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://assets-global.website-files.com/653dcbdd718cbd5c51c4dbed/653f5e8549a0fbf7dc295818_IMG_20190205_193746-p-500.jpg" alt="Jiahe(Paddy) ZHAO"></figure><p class="title is-size-4 is-block" style="line-height:inherit;margin-bottom:0.5rem;">Jiahe(Paddy) ZHAO</p><p class="is-block" style="font-style:Italic;font-size:0.8rem;">Artificial Intelligence</p><p class="is-block" style="font-style:Italic;font-size:0.8rem;">Machine Learning</p><p class="is-block" style="font-style:Italic;font-size:0.8rem;">Deep Learning</p><p class="is-block" style="font-style:Italic;font-size:0.8rem;">Computer Science</p><p class="is-block" style="font-style:Italic;font-size:0.8rem;">Information Technology</p><p class="is-size-6 is-flex justify-content-center" style="margin-top:0.5rem;"><i class="fas fa-map-marker-alt mr-1"></i><span>Hebei, CHINA</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading widget-profile-article-fontsize-style title-font-style">Posts</p><a href="/archives"><p class="title widget-profile-number-style">14</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading widget-profile-article-fontsize-style title-font-style">Categories</p><a href="/categories"><p class="title widget-profile-number-style">6</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading widget-profile-article-fontsize-style title-font-style">Tags</p><a href="/tags"><p class="title widget-profile-number-style">62</p></a></div></div></nav><div class="level"><a class="level-item fake-button-style is-rounded widget-profile-follow-button-style title-font-style" href="mailto:jiahe.zhao@uq.net.au?body=Hello%2C%0A%0AIt%20would%20be%20greatly%20appreciated%20if%20you%20could%20send%20me%20an%20email%20using%20your%20personal%20email%20client%20rather%20than%20the%20current%20HTML%20popup%20one%2C%20as%20I%20may%20not%20receive%20your%20message.%0A%0AThank%20you." target="_blank" rel="noopener"> <i class="fa fa-envelope" style="font-size:16.75px;margin-right:0.6rem;"></i>Email Me</a><span>  </span><a class="level-item fake-button-style is-rounded widget-profile-follow-button-style title-font-style" href="https://www.linkedin.com/in/jiahe-paddy-zhao-213b24300" target="_blank" rel="noopener"> <i class="fa-brands fa-linkedin color-linkedin" style="font-size:16.75px;margin-right:0.6rem;"></i>LinkedIn</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless widget-profile-logo-style" target="_blank" rel="noopener" title="Github" href="https://github.com/paddyzz"><i class="fa-brands fa-github color-github"></i></a><a class="level-item button is-transparent is-marginless widget-profile-logo-style" target="_blank" rel="noopener" title="Kaggle" href="https://www.kaggle.com/paddyzhao/"><i class="fa-brands fa-kaggle color-kaggle"></i></a><a class="level-item button is-transparent is-marginless widget-profile-logo-style" target="_blank" rel="noopener" title="Facebook" href="https://www.facebook.com/profile.php?id=100087961621533"><i class="fa-brands fa-facebook-f color-facebook"></i></a><a class="level-item button is-transparent is-marginless widget-profile-logo-style" target="_blank" rel="noopener" title="X" href="https://x.com/paddyzhao150568"><i class="fa-brands fa-x-twitter color-X"></i></a><a class="level-item button is-transparent is-marginless widget-profile-logo-style" target="_blank" rel="noopener" title="Discord" href="https://discord.com/users/1001308889062576139"><i class="fa-brands fa-discord color-discord"></i></a></div></div></div><div class="card widget is-sticky" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Introduction"><span class="level-left"><span class="level-item">1</span><span class="level-item">Introduction</span></span></a></li><li><a class="level is-mobile" href="#One-Hot-Encoding"><span class="level-left"><span class="level-item">2</span><span class="level-item">One-Hot Encoding</span></span></a></li><li><a class="level is-mobile" href="#Word2Vec-Word-Embeddings"><span class="level-left"><span class="level-item">3</span><span class="level-item">Word2Vec Word Embeddings</span></span></a></li><li><a class="level is-mobile" href="#Convolutional-Kernel"><span class="level-left"><span class="level-item">4</span><span class="level-item">Convolutional Kernel</span></span></a></li><li><a class="level is-mobile" href="#Convolution-on-Word-Sequences"><span class="level-left"><span class="level-item">5</span><span class="level-item">Convolution on Word Sequences</span></span></a></li><li><a class="level is-mobile" href="#Recognizing-General-Patterns"><span class="level-left"><span class="level-item">6</span><span class="level-item">Recognizing General Patterns</span></span></a></li><li><a class="level is-mobile" href="#1D-Convolution"><span class="level-left"><span class="level-item">7</span><span class="level-item">1D Convolution</span></span></a></li><li><a class="level is-mobile" href="#Multiple-Kernels"><span class="level-left"><span class="level-item">8</span><span class="level-item">Multiple Kernels</span></span></a></li><li><a class="level is-mobile" href="#Maxpooling-Over-Time"><span class="level-left"><span class="level-item">9</span><span class="level-item">Maxpooling Over Time</span></span></a></li><li><a class="level is-mobile" href="#Summarization"><span class="level-left"><span class="level-item">10</span><span class="level-item">Summarization</span></span></a></li><li><a class="level is-mobile" href="#References"><span class="level-left"><span class="level-item">11</span><span class="level-item">References</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><!--!--><!--!--><div class="column-right-shadow is-hidden-widescreen"></div></div></div></div></section><footer class="footer" style="padding:2rem 0.75rem 2rem;"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/images/favicon.png" alt="Paddy - Paddy&#039;s Log Book" height="28"></a><p class="is-size-7"><span>&copy; 2023-2024 Paddy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a> &amp; <a href="/" target="_blank" rel="noopener">Paddy&#039;s FE Tech</a><br><span id="busuanzi_container_page_uv">Visited by <span id="busuanzi_value_page_uv">88</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent article_license_logo is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent article_license_logo is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent article_license_logo is-large" target="_blank" rel="noopener" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><script src="/js/universe.js"></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script><script type="text/javascript" src="/js/gsap_progressbar.js"></script><script type="text/javascript" src="/js/night.js"></script></body></html>