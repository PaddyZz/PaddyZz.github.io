<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Neural machine translation - Paddy - Paddy&#039;s Log Book</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Paddy - Paddy&#039;s Log Book"><meta name="msapplication-TileImage" content="/images/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Paddy - Paddy&#039;s Log Book"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Neural machine translation with basic Seq-to-Seq Transformer Architecture and Keras"><meta property="og:type" content="blog"><meta property="og:title" content="Neural machine translation"><meta property="og:url" content="http://paddyzz.github.io/projects/neural_machine_translation/"><meta property="og:site_name" content="Paddy - Paddy&#039;s Log Book"><meta property="og:description" content="Neural machine translation with basic Seq-to-Seq Transformer Architecture and Keras"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://paddyzz.github.io/images/assets/neural_machine_translation/image.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/neural_machine_translation/image%201.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/neural_machine_translation/image%202.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/neural_machine_translation/image%203.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/neural_machine_translation/image%204.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/neural_machine_translation/image%205.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/neural_machine_translation/image%206.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/neural_machine_translation/image%207.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/neural_machine_translation/image%208.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/neural_machine_translation/image%209.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/neural_machine_translation/image%2010.png"><meta property="article:published_time" content="2024-05-30T16:00:00.000Z"><meta property="article:modified_time" content="2024-10-23T16:00:00.000Z"><meta property="article:author" content="Paddy"><meta property="article:tag" content="DL"><meta property="article:tag" content="NLP"><meta property="article:tag" content="ML"><meta property="article:tag" content="PROJECTS"><meta property="article:tag" content="Python"><meta property="article:tag" content="Seq-to-Seq"><meta property="article:tag" content="Tensorflow"><meta property="article:tag" content="Keras"><meta property="article:tag" content="transformer"><meta property="article:tag" content="tokenizer"><meta property="article:tag" content="self-attention"><meta property="article:tag" content="translation"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://paddyzz.github.io/images/assets/neural_machine_translation/image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://paddyzz.github.io/projects/neural_machine_translation/"},"headline":"Neural machine translation","image":["http://paddyzz.github.io/images/assets/neural_machine_translation/image.png","http://paddyzz.github.io/images/assets/neural_machine_translation/image%201.png","http://paddyzz.github.io/images/assets/neural_machine_translation/image%202.png","http://paddyzz.github.io/images/assets/neural_machine_translation/image%203.png","http://paddyzz.github.io/images/assets/neural_machine_translation/image%204.png","http://paddyzz.github.io/images/assets/neural_machine_translation/image%205.png","http://paddyzz.github.io/images/assets/neural_machine_translation/image%206.png","http://paddyzz.github.io/images/assets/neural_machine_translation/image%207.png","http://paddyzz.github.io/images/assets/neural_machine_translation/image%208.png","http://paddyzz.github.io/images/assets/neural_machine_translation/image%209.png","http://paddyzz.github.io/images/assets/neural_machine_translation/image%2010.png"],"datePublished":"2024-05-30T16:00:00.000Z","dateModified":"2024-10-23T16:00:00.000Z","author":{"@type":"Person","name":"Paddy"},"publisher":{"@type":"Organization","name":"Paddy - Paddy's Log Book","logo":{"@type":"ImageObject","url":"http://paddyzz.github.io/images/favicon.png"}},"description":"Neural machine translation with basic Seq-to-Seq Transformer Architecture and Keras"}</script><link rel="canonical" href="http://paddyzz.github.io/projects/neural_machine_translation/"><link rel="icon" href="/images/favicon.png"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.2/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><meta name="follow.it-verification-code" content="SdbKP9l6XayZRBYCuvDS"><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body class="is-3-column"><script type="text/javascript" src="/js/svg-inject.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/gsap@3.3.4/dist/gsap.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/gsap@3.3.4/dist/ScrollTrigger.js"></script><progress max="100" value="0"></progress><canvas id="universe"></canvas><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/images/favicon.png" alt="Paddy - Paddy&#039;s Log Book" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item navbar-item-home" href="/">PADDY&#039;S LOG BOOK</a><a class="navbar-item" href="/curriculum/">CURRICULUM</a><a class="navbar-item" href="/Certificates/">CERTIFICATES</a><a class="navbar-item" href="/blogs/">BLOGS</a><a class="navbar-item" href="/projects/">PROJECTS</a><a class="navbar-item" href="/life/">LIFE</a><a class="navbar-item" href="/archives/">ARCHIVES</a><a class="navbar-item" href="/categories/">CATEGORIES</a><a class="navbar-item" href="/tags/">TAGS</a><a class="navbar-item" href="/faqs/">FAQS</a></div><div class="navbar-end"><a class="navbar-item navbar-item-logo" id="star-nav" title="Nox Tenebratio!" style="opacity:0;display:none;userSelect:none;" href="javascript:;"><i class="fa fa-star-and-crescent" id="star-icon" style="font-size:17.75px"></i></a><a class="navbar-item navbar-item-logo night" id="night-nav" title="Nox!" href="javascript:;"><i class="fas fa-moon" id="night-icon" style="font-size:16.75px"></i></a><a class="navbar-item navbar-item-logo" id="night-nav" title="Email" href="mailto:jiahe.zhao@uq.net.au?body=Hello%2C%0A%0AIt%20would%20be%20greatly%20appreciated%20if%20you%20could%20send%20me%20an%20email%20using%20your%20personal%20email%20client%20rather%20than%20the%20current%20HTML%20popup%20one%2C%20as%20I%20may%20not%20receive%20your%20message.%0A%0AThank%20you."><i class="fas fa-envelope" style="font-size:16.75px"></i></a><a class="navbar-item navbar-item-logo" title="Gitter" target="_blank" rel="noopener" href="https://app.gitter.im/#/room/#Paddy/Community:gitter.im"><i class="fab fa-gitter" style="font-size:16.75px"></i></a><a class="navbar-item navbar-item-logo" title="Element" target="_blank" rel="noopener" href="https://app.element.io/#/room/#Paddy/Community:gitter.im"><svg version="1.0" id="svg-element" xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 225 225"><g fill="#4a4a4a"><path d="M91 1c-11 3-13 19-3 24l11 2c32 3 56 27 59 59 0 9 2 12 6 15 7 4 15 2 19-5l1-13c-1-11-3-19-9-31A87 87 0 0 0 91 1zM73 43a90 90 0 0 0-72 91c3 11 19 13 24 3l2-11c3-32 27-56 59-59 9 0 12-2 15-6 4-7 2-15-5-19H73zM208 82c-7 2-9 6-10 17-3 32-27 56-59 59-9 0-12 2-15 6-4 7-1 15 5 19l13 1c12-1 20-3 33-9a86 86 0 0 0 49-84c-1-4-6-9-8-9h-8zM47 124c-5 3-6 7-6 18 1 12 3 20 10 33 13 28 39 46 71 49 10 1 14 0 18-3 6-6 4-17-3-21l-11-2c-32-3-56-27-59-59l-2-10c-2-4-7-7-11-7l-7 2z"></path></g></svg></a><a class="navbar-item navbar-item-logo" title="GitHub" target="_blank" rel="noopener" href="https://github.com/paddyzz"><i class="fab fa-github" style="font-size:16.75px"></i></a><a class="navbar-item navbar-item-logo" title="Linkedin" target="_blank" rel="noopener" href="https://www.linkedin.com/in/jiahe-paddy-zhao-213b24300"><i class="fab fa-linkedin" style="font-size:19.75px"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item navbar-item-logo search" title="Search For It !" href="javascript:;"><i class="fa-brands fa-searchengin" style="font-size:19.75px"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-3 is-size-4-mobile title-font-style">Neural machine translation</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item author-capitalize"> Paddy </span><span class="level-item"><i class="far fa-calendar-alt"></i>&nbsp;<time dateTime="2024-05-30T16:00:00.000Z" title="31/05/2024, 12:00:00 am">31-05-2024</time></span><span class="level-item"><i class="far fa-calendar-check"></i>&nbsp;<time dateTime="2024-10-23T16:00:00.000Z" title="24/10/2024, 12:00:00 am">24-10-2024</time></span><span class="level-item"><i class="far fa-folder-open has-text-grey"></i><span> </span><a class="link-muted" href="/projects/">projects</a></span><span class="level-item"> <i class="far fa-clock"></i> <span> </span> 25 minutes read<span> </span>( About 3793 words)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv">0</span>&nbsp;visits</span></div></div><article class="message is-info" style="margin-top:1.5rem;font-style:oblique"><div class="message-body">Neural machine translation with basic Seq-to-Seq Transformer Architecture and Keras</div></article><div class="content card-content-font-style" style="margin-top:1.5rem;margin-bottom:1rem"><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>In this blog, we will create and train a <a target="_blank" rel="noopener" href="https://developers.google.com/machine-learning/glossary#sequence-to-sequence-task">sequence-to-sequence</a> <a target="_blank" rel="noopener" href="https://developers.google.com/machine-learning/glossary#Transformer">Transformer</a> model to translate <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https://www.tensorflow.org/datasets/catalog/ted_hrlr_translate%23ted_hrlr_translatept_to_en">Portuguese into English</a>.</p>
<p>Transformers are deep neural networks that replace CNNs and RNNs with <a target="_blank" rel="noopener" href="https://developers.google.com/machine-learning/glossary#self-attention">self-attention</a>. Self-attention allows Transformers to easily transmit information across the input sequences.</p>
<p>As suggested in the <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Google AI Blog post</a>:</p>
<blockquote>
<p>Neural networks for machine translation typically contain an encoder reading the input sentence and generating a representation of it. A decoder then generates the output sentence word by word while consulting the representation generated by the encoder. The Transformer starts by generating initial representations, or embeddings, for each word… Then, using self-attention, it aggregates information from all of the other words, generating a new representation per word informed by the entire context, represented by the filled balls. This step is then repeated multiple times in parallel for all words, successively generating new representations.</p>
</blockquote>
<p>let’s dive into it!</p>
<h2 id="Set-up"><a href="#Set-up" class="headerlink" title="Set up"></a>Set up</h2><p>Begin by installing <a target="_blank" rel="noopener" href="https://tensorflow.org/datasets">TensorFlow Datasets</a> for loading the dataset and <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https://www.tensorflow.org/text">TensorFlow Text</a> for text preprocessing:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># google colab</span><br><span class="line"># Install the most re version of TensorFlow to use the improved</span><br><span class="line"># masking support for `tf.keras.layers.MultiHeadAttention`.</span><br><span class="line">!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2</span><br><span class="line">!pip uninstall -y -q tensorflow keras tensorflow-estimator tensorflow-text</span><br><span class="line">!pip install protobuf~=3.20.3</span><br><span class="line">!pip install -q tensorflow_datasets</span><br><span class="line">!pip install -q -U tensorflow-text tensorflow</span><br><span class="line">import logging</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">import tensorflow_datasets as tfds</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">import tensorflow_text</span><br><span class="line">!pip install datasets</span><br></pre></td></tr></table></figure>

<h2 id="Data-handling"><a href="#Data-handling" class="headerlink" title="Data handling"></a>Data handling</h2><h3 id="Download-the-dataset"><a href="#Download-the-dataset" class="headerlink" title="Download the dataset"></a>Download the dataset</h3><p>Use TensorFlow Datasets to load the <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https://www.tensorflow.org/datasets/catalog/ted_hrlr_translate%23ted_hrlr_translatept_to_en">Portuguese-English translation dataset</a>TED Talks Open Translation Project. This dataset contains approximately 52,000 training, 1,200 validation and 1,800 test examples.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en',</span><br><span class="line">                               with_info=True,</span><br><span class="line">                               as_supervised=True)</span><br><span class="line"></span><br><span class="line">train_examples, val_examples = examples['train'], examples['validation']</span><br></pre></td></tr></table></figure>

<p>after we have loaded the dataset, we will tokenize the text, so that each element is represented as a <a target="_blank" rel="noopener" href="https://developers.google.com/machine-learning/glossary#token">token</a> or token ID (a numeric representation).</p>
<h3 id="Set-up-the-tokenizer"><a href="#Set-up-the-tokenizer" class="headerlink" title="Set up the tokenizer"></a>Set up the tokenizer</h3><p>Download, extract, and import the <code>saved_model</code>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model_name = 'ted_hrlr_translate_pt_en_converter'</span><br><span class="line">tf.keras.utils.get_file(</span><br><span class="line">    f'{model_name}.zip',</span><br><span class="line">    f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',</span><br><span class="line">    cache_dir='.', cache_subdir='', extract=True</span><br><span class="line">)</span><br><span class="line">tokenizers = tf.saved_model.load(f'{model_name}_extracted/{model_name}')</span><br></pre></td></tr></table></figure>

<h3 id="Set-up-a-data-pipeline-with-tf-data"><a href="#Set-up-a-data-pipeline-with-tf-data" class="headerlink" title="Set up a data pipeline with tf.data"></a><strong>Set up a data pipeline with <code>tf.data</code></strong></h3><p>The following function takes batches of text as input, and converts them to a format suitable for training.</p>
<ol>
<li>It tokenizes them into ragged batches.</li>
<li>It trims each to be no longer than <code>MAX_TOKENS</code>.</li>
<li>It splits the target (English) tokens into inputs and labels. These are shifted by one step so that at each input location the <code>label</code> is the id of the next token.</li>
<li>It converts the <code>RaggedTensor</code> to padded dense <code>Tensor</code>.</li>
<li>It returns an <code>(inputs, labels)</code> pair.</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">MAX_TOKENS=128</span><br><span class="line">def prepare_batch(pt, en):</span><br><span class="line">    pt = tokenizers.pt.tokenize(pt)      # Output is ragged.</span><br><span class="line">    pt = pt[:, :MAX_TOKENS]    # Trim to MAX_TOKENS.</span><br><span class="line">    pt = pt.to_tensor()  # Convert to 0-padded dense Tensor</span><br><span class="line"></span><br><span class="line">    en = tokenizers.en.tokenize(en)</span><br><span class="line">    en = en[:, :(MAX_TOKENS+1)]</span><br><span class="line">    en_inputs = en[:, :-1].to_tensor()  # Drop the [END] tokens</span><br><span class="line">    en_labels = en[:, 1:].to_tensor()   # Drop the [START] tokens</span><br><span class="line"></span><br><span class="line">    return (pt, en_inputs), en_labels</span><br></pre></td></tr></table></figure>

<p>The function below converts a dataset of text examples into data of batches for training.</p>
<ol>
<li>It tokenizes the text, and filters out the sequences that are too long. (The <code>batch</code>/<code>unbatch</code> is included because the tokenizer is much more efficient on large batches).</li>
<li>The <code>cache</code> method ensures that that work is only executed once.</li>
<li>Then <code>shuffle</code> and, <code>dense_to_ragged_batch</code> randomize the order and assemble batches of examples.</li>
<li>Finally <code>prefetch</code> runs the dataset in parallel with the model to ensure that data is available when needed.</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">BUFFER_SIZE = 20000</span><br><span class="line">BATCH_SIZE = 64</span><br><span class="line">def make_batches(ds):</span><br><span class="line">  return (</span><br><span class="line">      ds</span><br><span class="line">      .shuffle(BUFFER_SIZE)</span><br><span class="line">      .batch(BATCH_SIZE)</span><br><span class="line">      .map(prepare_batch, tf.data.AUTOTUNE)</span><br><span class="line">      .prefetch(buffer_size=tf.data.AUTOTUNE))</span><br><span class="line"></span><br><span class="line"># Create training and validation set batches.</span><br><span class="line">train_batches = make_batches(train_examples)</span><br><span class="line">val_batches = make_batches(val_examples)     </span><br></pre></td></tr></table></figure>

<h2 id="Define-the-components"><a href="#Define-the-components" class="headerlink" title="Define the components"></a>Define the components</h2><p>we will start to implement the components of a Transformer as a standard sequence-to-sequence model with an encoder and a decoder.</p>
<figcaption align="center">
  <img src="../../images/assets/neural_machine_translation/image.png">
  <figcaption>Figure 1. The original transformer diagram.</figcaption>
</figcaption>

<h3 id="The-embedding-and-positional-encoding-layer"><a href="#The-embedding-and-positional-encoding-layer" class="headerlink" title="The embedding and positional encoding layer"></a><strong>The embedding and positional encoding layer</strong></h3><p>The inputs to both the encoder and decoder use the same embedding and positional encoding logic.</p>
<figcaption align="center">
  <img src="../../images/assets/neural_machine_translation/image 1.png">
</figcaption>

<p>A Transformer adds a “Positional Encoding” to the embedding vectors. It uses a set of sines and cosines at different frequencies (across the sequence). By definition nearby elements will have similar position encodings.</p>
<p>Using the following formula for calculating the positional encoding:</p>
<p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.8ex;" xmlns="http://www.w3.org/2000/svg" width="32.866ex" height="2.934ex" role="img" focusable="false" viewBox="0 -943.3 14527 1296.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="msub" transform="translate(751,0)"><g data-mml-node="mi"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="TeXAtom" transform="translate(771,-176.7) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(892,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(1377,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mo" transform="translate(1846,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mn" transform="translate(2124,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(2624,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(2969,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g><g data-mml-node="mo" transform="translate(4224.2,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(5280,0)"><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(394,0)"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(672,0)"></path></g><g data-mml-node="mo" transform="translate(6508,0)"><path data-c="2061" d=""></path></g><g data-mml-node="mo" transform="translate(6508,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(6897,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(7400,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(7885,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(8354,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path></g></g><g data-mml-node="msup" transform="translate(8854,0)"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(1000,0)"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(1500,0)"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(2000,0)"></path></g><g data-mml-node="TeXAtom" transform="translate(2533,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(500,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(845,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path></g></g><g data-mml-node="msub" transform="translate(1345,0)"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="TeXAtom" transform="translate(553,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(878,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(1363,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(1883,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(2349,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g></g></g></g><g data-mml-node="mo" transform="translate(14138,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></p>
<p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.8ex;" xmlns="http://www.w3.org/2000/svg" width="35.16ex" height="2.934ex" role="img" focusable="false" viewBox="0 -943.3 15540.6 1296.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="msub" transform="translate(751,0)"><g data-mml-node="mi"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="TeXAtom" transform="translate(771,-176.7) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(892,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(1377,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mo" transform="translate(1846,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mn" transform="translate(2124,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(2624,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(2969,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(3747,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(4247,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g><g data-mml-node="mo" transform="translate(5127.9,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(6183.7,0)"><path data-c="63" d="M370 305T349 305T313 320T297 358Q297 381 312 396Q317 401 317 402T307 404Q281 408 258 408Q209 408 178 376Q131 329 131 219Q131 137 162 90Q203 29 272 29Q313 29 338 55T374 117Q376 125 379 127T395 129H409Q415 123 415 120Q415 116 411 104T395 71T366 33T318 2T249 -11Q163 -11 99 53T34 214Q34 318 99 383T250 448T370 421T404 357Q404 334 387 320Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(444,0)"></path><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" transform="translate(944,0)"></path></g><g data-mml-node="mo" transform="translate(7521.7,0)"><path data-c="2061" d=""></path></g><g data-mml-node="mo" transform="translate(7521.7,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(7910.7,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(8413.7,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(8898.7,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(9367.7,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path></g></g><g data-mml-node="msup" transform="translate(9867.7,0)"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(1000,0)"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(1500,0)"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(2000,0)"></path></g><g data-mml-node="TeXAtom" transform="translate(2533,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(500,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(845,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path></g></g><g data-mml-node="msub" transform="translate(1345,0)"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="TeXAtom" transform="translate(553,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(878,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(1363,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(1883,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(2349,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g></g></g></g><g data-mml-node="mo" transform="translate(15151.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></p>
<p>The function using the vectors of sines and cosines  concatenated simply to implement it</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def positional_encoding(length, depth):</span><br><span class="line">  depth = depth/2</span><br><span class="line"></span><br><span class="line">  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)</span><br><span class="line">  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)</span><br><span class="line"></span><br><span class="line">  angle_rates = 1 / (10000**depths)         # (1, depth)</span><br><span class="line">  angle_rads = positions * angle_rates      # (pos, depth)</span><br><span class="line"></span><br><span class="line">  pos_encoding = np.concatenate(</span><br><span class="line">      [np.sin(angle_rads), np.cos(angle_rads)],</span><br><span class="line">      axis=-1)</span><br><span class="line"></span><br><span class="line">  return tf.cast(pos_encoding, dtype=tf.float32)</span><br></pre></td></tr></table></figure>

<p>The position encoding function is a stack of sines and cosines that vibrate at different frequencies depending on their location along the depth of the embedding vector. They vibrate across the position axis.</p>
<p>Creating a <code>PositionEmbedding</code> layer that looks-up a token’s embedding vector and adds the position vector:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class PositionalEmbedding(tf.keras.layers.Layer):</span><br><span class="line">  def __init__(self, vocab_size, d_model):</span><br><span class="line">    super().__init__()</span><br><span class="line">    self.d_model = d_model</span><br><span class="line">    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)</span><br><span class="line">    self.pos_encoding = positional_encoding(length=2048, depth=d_model)</span><br><span class="line"></span><br><span class="line">  def compute_mask(self, *args, **kwargs):</span><br><span class="line">    return self.embedding.compute_mask(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">  def call(self, x):</span><br><span class="line">    length = tf.shape(x)[1]</span><br><span class="line">    x = self.embedding(x)</span><br><span class="line">    # This factor sets the relative scale of the embedding and positonal_encoding.</span><br><span class="line">    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))</span><br><span class="line">    x = x + self.pos_encoding[tf.newaxis, :length, :]</span><br><span class="line">    return x</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="Add-and-normalise"><a href="#Add-and-normalise" class="headerlink" title="Add and normalise"></a><strong>Add and normalise</strong></h3><figcaption align="center">
  <img src="../../images/assets/neural_machine_translation/image 2.png">
</figcaption>

<p>These “Add &amp; Norm” blocks are scattered throughout the model. Each one joins a residual connection and runs the result through a <code>LayerNormalization</code> layer.</p>
<h3 id="The-base-attention-layer"><a href="#The-base-attention-layer" class="headerlink" title="The base attention layer"></a><strong>The base attention layer</strong></h3><p>Attention layers are used throughout the model. These are all identical except for how the attention is configured. Each one contains a <code>layers.MultiHeadAttention</code>, a <code>layers.LayerNormalization</code> and a <code>layers.Add</code></p>
<p>. And we will get started from a simple base class that just contains the component layers</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">class BaseAttention(tf.keras.layers.Layer):</span><br><span class="line">  def __init__(self, **kwargs):</span><br><span class="line">    super().__init__()</span><br><span class="line">    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)</span><br><span class="line">    self.layernorm = tf.keras.layers.LayerNormalization()</span><br><span class="line">    self.add = tf.keras.layers.Add()</span><br></pre></td></tr></table></figure>

<h3 id="The-cross-attention-layer"><a href="#The-cross-attention-layer" class="headerlink" title="The cross attention layer"></a><strong>The cross attention layer</strong></h3><p>At the literal center of the Transformer is the cross-attention layer. This layer connects the encoder and decoder. This layer is the most straight-forward use of attention in the model, it performs the same task as the attention block</p>
<figcaption align="center">
  <img src="../../images/assets/neural_machine_translation/image 3.png">
</figcaption>

<p>To implement this, we pass the target sequence <code>x</code> as the <code>query</code> and the <code>context</code> sequence as the <code>key/value</code> when calling the <code>mha</code> layer:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class CrossAttention(BaseAttention):</span><br><span class="line">  def call(self, x, context):</span><br><span class="line">    attn_output, attn_scores = self.mha(</span><br><span class="line">        query=x,</span><br><span class="line">        key=context,</span><br><span class="line">        value=context,</span><br><span class="line">        return_attention_scores=True)</span><br><span class="line"></span><br><span class="line">    # Cache the attention scores for plotting later.</span><br><span class="line">    self.last_attn_scores = attn_scores</span><br><span class="line"></span><br><span class="line">    x = self.add([x, attn_output])</span><br><span class="line">    x = self.layernorm(x)</span><br><span class="line"></span><br><span class="line">    return x</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test out the layer</span></span><br><span class="line">sample_ca = CrossAttention(num_heads=<span class="number">2</span>, key_dim=<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(pt_emb.shape)</span><br><span class="line"><span class="built_in">print</span>(en_emb.shape)</span><br><span class="line"><span class="built_in">print</span>(sample_ca(en_emb, pt_emb).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># result </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(64, 75, 512)</span></span><br><span class="line"><span class="string">(64, 53, 512)</span></span><br><span class="line"><span class="string">(64, 53, 512)</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h3 id="The-global-self-attention-layer"><a href="#The-global-self-attention-layer" class="headerlink" title="The global self-attention layer"></a><strong>The global self-attention layer</strong></h3><p>This layer is responsible for processing the context sequence, and propagating information along its length:</p>
<figcaption align="center">
  <img src="../../images/assets/neural_machine_translation/image 4.png">
</figcaption>

<p>To implement this layer we just need to pass the target sequence, <code>x</code>, as both the <code>query</code>, and <code>value</code> arguments to the <code>mha</code> layer:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">class GlobalSelfAttention(BaseAttention):</span><br><span class="line">  def call(self, x):</span><br><span class="line">    attn_output = self.mha(</span><br><span class="line">        query=x,</span><br><span class="line">        value=x,</span><br><span class="line">        key=x)</span><br><span class="line">    x = self.add([x, attn_output])</span><br><span class="line">    x = self.layernorm(x)</span><br><span class="line">    return x</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test out the layer</span></span><br><span class="line">sample_gsa = GlobalSelfAttention(num_heads=<span class="number">2</span>, key_dim=<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(pt_emb.shape)</span><br><span class="line"><span class="built_in">print</span>(sample_gsa(pt_emb).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># result </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(64, 75, 512)</span></span><br><span class="line"><span class="string">(64, 75, 512)</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h3 id="The-causal-self-attention-layer"><a href="#The-causal-self-attention-layer" class="headerlink" title="The causal self-attention layer"></a><strong>The causal self-attention layer</strong></h3><p>This layer does a similar job as the global self-attention layer, for the output sequence:</p>
<figcaption align="center">
  <img src="../../images/assets/neural_machine_translation/image 5.png">
</figcaption>

<p>To build a causal self-attention layer, we need to use an appropriate mask when computing the attention scores and summing the attention <code>value</code>s. And we can solve this pass <code>use_causal_mask = True</code> to the <code>MultiHeadAttention</code> layer </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class CausalSelfAttention(BaseAttention):</span><br><span class="line">  def call(self, x):</span><br><span class="line">    attn_output = self.mha(</span><br><span class="line">        query=x,</span><br><span class="line">        value=x,</span><br><span class="line">        key=x,</span><br><span class="line">        use_causal_mask = True)</span><br><span class="line">    x = self.add([x, attn_output])</span><br><span class="line">    x = self.layernorm(x)</span><br><span class="line">    return x</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test out the layer</span></span><br><span class="line">sample_csa = CausalSelfAttention(num_heads=<span class="number">2</span>, key_dim=<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(en_emb.shape)</span><br><span class="line"><span class="built_in">print</span>(sample_csa(en_emb).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># result </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(64, 53, 512)</span></span><br><span class="line"><span class="string">(64, 53, 512)</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h3 id="The-feed-forward-network"><a href="#The-feed-forward-network" class="headerlink" title="The feed forward network"></a><strong>The feed forward network</strong></h3><p>The transformer also includes this point-wise feed-forward network in both the encoder and decoder:</p>
<p>The network consists of two linear layers (<code>tf.keras.layers.Dense</code>) with a ReLU activation in-between, and a dropout layer. As with the attention layers the code here also includes the residual connection and normalization:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">class FeedForward(tf.keras.layers.Layer):</span><br><span class="line">  def __init__(self, d_model, dff, dropout_rate=0.1):</span><br><span class="line">    super().__init__()</span><br><span class="line">    self.seq = tf.keras.Sequential([</span><br><span class="line">      tf.keras.layers.Dense(dff, activation='relu'),</span><br><span class="line">      tf.keras.layers.Dense(d_model),</span><br><span class="line">      tf.keras.layers.Dropout(dropout_rate)</span><br><span class="line">    ])</span><br><span class="line">    self.add = tf.keras.layers.Add()</span><br><span class="line">    self.layer_norm = tf.keras.layers.LayerNormalization()</span><br><span class="line"></span><br><span class="line">  def call(self, x):</span><br><span class="line">    x = self.add([x, self.seq(x)])</span><br><span class="line">    x = self.layer_norm(x)</span><br><span class="line">    return x</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test out the layer</span></span><br><span class="line">sample_ffn = FeedForward(<span class="number">512</span>, <span class="number">2048</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(en_emb.shape)</span><br><span class="line"><span class="built_in">print</span>(sample_ffn(en_emb).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># result </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(64, 53, 512)</span></span><br><span class="line"><span class="string">(64, 53, 512)</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h3 id="The-encoder-layer"><a href="#The-encoder-layer" class="headerlink" title="The encoder layer"></a><strong>The encoder layer</strong></h3><p>The encoder contains a stack of <code>N</code> encoder layers. Where each <code>EncoderLayer</code> contains a <code>GlobalSelfAttention</code> and <code>FeedForward</code> layer:</p>
<figcaption align="center">
  <img src="../../images/assets/neural_machine_translation/image 6.png">
</figcaption>

<p>Here is the definition of the <code>EncoderLayer</code>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class EncoderLayer(tf.keras.layers.Layer):</span><br><span class="line">  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):</span><br><span class="line">    super().__init__()</span><br><span class="line"></span><br><span class="line">    self.self_attention = GlobalSelfAttention(</span><br><span class="line">        num_heads=num_heads,</span><br><span class="line">        key_dim=d_model,</span><br><span class="line">        dropout=dropout_rate)</span><br><span class="line"></span><br><span class="line">    self.ffn = FeedForward(d_model, dff)</span><br><span class="line"></span><br><span class="line">  def call(self, x):</span><br><span class="line">    x = self.self_attention(x)</span><br><span class="line">    x = self.ffn(x)</span><br><span class="line">    return x</span><br></pre></td></tr></table></figure>

<p>And a quick test again, the output will have the same shape as the input</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test out the layer</span></span><br><span class="line">sample_encoder_layer = EncoderLayer(d_model=<span class="number">512</span>, num_heads=<span class="number">8</span>, dff=<span class="number">2048</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(pt_emb.shape)</span><br><span class="line"><span class="built_in">print</span>(sample_encoder_layer(pt_emb).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># result </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(64, 75, 512)</span></span><br><span class="line"><span class="string">(64, 75, 512)</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h3 id="The-encoder"><a href="#The-encoder" class="headerlink" title="The encoder"></a><strong>The encoder</strong></h3><figcaption align="center">
  <img src="../../images/assets/neural_machine_translation/image 7.png">
</figcaption>

<p>The encoder consists of:</p>
<ul>
<li>A <code>PositionalEmbedding</code> layer at the input.</li>
<li>A stack of <code>EncoderLayer</code> layers.</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">class Encoder(tf.keras.layers.Layer):</span><br><span class="line">  def __init__(self, *, num_layers, d_model, num_heads,</span><br><span class="line">               dff, vocab_size, dropout_rate=0.1):</span><br><span class="line">    super().__init__()</span><br><span class="line"></span><br><span class="line">    self.d_model = d_model</span><br><span class="line">    self.num_layers = num_layers</span><br><span class="line"></span><br><span class="line">    self.pos_embedding = PositionalEmbedding(</span><br><span class="line">        vocab_size=vocab_size, d_model=d_model)</span><br><span class="line"></span><br><span class="line">    self.enc_layers = [</span><br><span class="line">        EncoderLayer(d_model=d_model,</span><br><span class="line">                     num_heads=num_heads,</span><br><span class="line">                     dff=dff,</span><br><span class="line">                     dropout_rate=dropout_rate)</span><br><span class="line">        for _ in range(num_layers)]</span><br><span class="line">    self.dropout = tf.keras.layers.Dropout(dropout_rate)</span><br><span class="line"></span><br><span class="line">  def call(self, x):</span><br><span class="line">    # `x` is token-IDs shape: (batch, seq_len)</span><br><span class="line">    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.</span><br><span class="line"></span><br><span class="line">    # Add dropout.</span><br><span class="line">    x = self.dropout(x)</span><br><span class="line"></span><br><span class="line">    for i in range(self.num_layers):</span><br><span class="line">      x = self.enc_layers[i](x)</span><br><span class="line"></span><br><span class="line">    return x  # Shape `(batch_size, seq_len, d_model)`.</span><br></pre></td></tr></table></figure>

<p>And test the encoder:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test out the layer</span></span><br><span class="line"><span class="comment"># Instantiate the encoder.</span></span><br><span class="line">sample_encoder = Encoder(num_layers=<span class="number">4</span>,</span><br><span class="line">                         d_model=<span class="number">512</span>,</span><br><span class="line">                         num_heads=<span class="number">8</span>,</span><br><span class="line">                         dff=<span class="number">2048</span>,</span><br><span class="line">                         vocab_size=<span class="number">8500</span>)</span><br><span class="line"></span><br><span class="line">sample_encoder_output = sample_encoder(pt, training=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(pt.shape)</span><br><span class="line"><span class="built_in">print</span>(sample_encoder_output.shape)  <span class="comment"># Shape `(batch_size, input_seq_len, d_model)`.</span></span><br><span class="line"><span class="comment"># result </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(64, 75)</span></span><br><span class="line"><span class="string">(64, 75, 512)</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h3 id="The-decoder-layer"><a href="#The-decoder-layer" class="headerlink" title="The decoder layer"></a><strong>The decoder layer</strong></h3><p>The decoder’s stack is slightly more complex, with each <code>DecoderLayer</code> containing a <code>CausalSelfAttention</code>, a <code>CrossAttention</code>, and a <code>FeedForward</code> layer:</p>
<figcaption align="center">
  <img src="../../images/assets/neural_machine_translation/image 8.png">
</figcaption>

<p>Implement it:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">class DecoderLayer(tf.keras.layers.Layer):</span><br><span class="line">  def __init__(self,</span><br><span class="line">               *,</span><br><span class="line">               d_model,</span><br><span class="line">               num_heads,</span><br><span class="line">               dff,</span><br><span class="line">               dropout_rate=0.1):</span><br><span class="line">    super(DecoderLayer, self).__init__()</span><br><span class="line"></span><br><span class="line">    self.causal_self_attention = CausalSelfAttention(</span><br><span class="line">        num_heads=num_heads,</span><br><span class="line">        key_dim=d_model,</span><br><span class="line">        dropout=dropout_rate)</span><br><span class="line"></span><br><span class="line">    self.cross_attention = CrossAttention(</span><br><span class="line">        num_heads=num_heads,</span><br><span class="line">        key_dim=d_model,</span><br><span class="line">        dropout=dropout_rate)</span><br><span class="line"></span><br><span class="line">    self.ffn = FeedForward(d_model, dff)</span><br><span class="line"></span><br><span class="line">  def call(self, x, context):</span><br><span class="line">    x = self.causal_self_attention(x=x)</span><br><span class="line">    x = self.cross_attention(x=x, context=context)</span><br><span class="line"></span><br><span class="line">    # Cache the last attention scores for plotting later</span><br><span class="line">    self.last_attn_scores = self.cross_attention.last_attn_scores</span><br><span class="line"></span><br><span class="line">    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.</span><br><span class="line">    return x</span><br></pre></td></tr></table></figure>

<p>Test the layer:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">sample_decoder_layer = DecoderLayer(d_model=<span class="number">512</span>, num_heads=<span class="number">8</span>, dff=<span class="number">2048</span>)</span><br><span class="line"></span><br><span class="line">sample_decoder_layer_output = sample_decoder_layer(</span><br><span class="line">    x=en_emb, context=pt_emb)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(en_emb.shape)</span><br><span class="line"><span class="built_in">print</span>(pt_emb.shape)</span><br><span class="line"><span class="built_in">print</span>(sample_decoder_layer_output.shape)  <span class="comment"># `(batch_size, seq_len, d_model)`</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># result</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(64, 53, 512)</span></span><br><span class="line"><span class="string">(64, 75, 512)</span></span><br><span class="line"><span class="string">(64, 53, 512)</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h3 id="The-decoder"><a href="#The-decoder" class="headerlink" title="The decoder"></a><strong>The decoder</strong></h3><p>Similar to the <code>Encoder</code>, the <code>Decoder</code> consists of a <code>PositionalEmbedding</code>, and a stack of <code>DecoderLayer</code></p>
<figcaption align="center">
  <img src="../../images/assets/neural_machine_translation/image 9.png">
</figcaption>

<p>Define the decoder by extending <code>tf.keras.layers.Layer</code>:  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">class Decoder(tf.keras.layers.Layer):</span><br><span class="line">  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,</span><br><span class="line">               dropout_rate=0.1):</span><br><span class="line">    super(Decoder, self).__init__()</span><br><span class="line"></span><br><span class="line">    self.d_model = d_model</span><br><span class="line">    self.num_layers = num_layers</span><br><span class="line"></span><br><span class="line">    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,</span><br><span class="line">                                             d_model=d_model)</span><br><span class="line">    self.dropout = tf.keras.layers.Dropout(dropout_rate)</span><br><span class="line">    self.dec_layers = [</span><br><span class="line">        DecoderLayer(d_model=d_model, num_heads=num_heads,</span><br><span class="line">                     dff=dff, dropout_rate=dropout_rate)</span><br><span class="line">        for _ in range(num_layers)]</span><br><span class="line"></span><br><span class="line">    self.last_attn_scores = None</span><br><span class="line"></span><br><span class="line">  def call(self, x, context):</span><br><span class="line">    # `x` is token-IDs shape (batch, target_seq_len)</span><br><span class="line">    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)</span><br><span class="line"></span><br><span class="line">    x = self.dropout(x)</span><br><span class="line"></span><br><span class="line">    for i in range(self.num_layers):</span><br><span class="line">      x  = self.dec_layers[i](x, context)</span><br><span class="line"></span><br><span class="line">    self.last_attn_scores = self.dec_layers[-1].last_attn_scores</span><br><span class="line"></span><br><span class="line">    # The shape of x is (batch_size, target_seq_len, d_model).</span><br><span class="line">    return x</span><br></pre></td></tr></table></figure>

<p>Test the decoder:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Instantiate the decoder.</span></span><br><span class="line">sample_decoder = Decoder(num_layers=<span class="number">4</span>,</span><br><span class="line">                         d_model=<span class="number">512</span>,</span><br><span class="line">                         num_heads=<span class="number">8</span>,</span><br><span class="line">                         dff=<span class="number">2048</span>,</span><br><span class="line">                         vocab_size=<span class="number">8000</span>)</span><br><span class="line"></span><br><span class="line">output = sample_decoder(</span><br><span class="line">    x=en,</span><br><span class="line">    context=pt_emb)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the shapes.</span></span><br><span class="line"><span class="built_in">print</span>(en.shape)</span><br><span class="line"><span class="built_in">print</span>(pt_emb.shape)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># result</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(64, 53)</span></span><br><span class="line"><span class="string">(64, 75, 512)</span></span><br><span class="line"><span class="string">(64, 53, 512)</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h3 id="The-Transformer"><a href="#The-Transformer" class="headerlink" title="The Transformer"></a><strong>The Transformer</strong></h3><p>Now we need to put  <code>Encoder</code> and <code>Decoder</code> together and add a final linear (<code>Dense</code>) layer which converts the resulting vector at each location into output token probabilities to finish the transformer model to be created.</p>
<figcaption align="center">
  <img src="../../images/assets/neural_machine_translation/image 10.png">
</figcaption>

<p>Create the <code>Transformer</code> by extending <code>tf.keras.Model</code>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">class Transformer(tf.keras.Model):</span><br><span class="line">  def __init__(self, *, num_layers, d_model, num_heads, dff,</span><br><span class="line">               input_vocab_size, target_vocab_size, dropout_rate=0.1):</span><br><span class="line">    super().__init__()</span><br><span class="line">    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,</span><br><span class="line">                           num_heads=num_heads, dff=dff,</span><br><span class="line">                           vocab_size=input_vocab_size,</span><br><span class="line">                           dropout_rate=dropout_rate)</span><br><span class="line"></span><br><span class="line">    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,</span><br><span class="line">                           num_heads=num_heads, dff=dff,</span><br><span class="line">                           vocab_size=target_vocab_size,</span><br><span class="line">                           dropout_rate=dropout_rate)</span><br><span class="line"></span><br><span class="line">    self.final_layer = tf.keras.layers.Dense(target_vocab_size)</span><br><span class="line"></span><br><span class="line">  def call(self, inputs):</span><br><span class="line">    # To use a Keras model with `.fit` you must pass all your inputs in the</span><br><span class="line">    # first argument.</span><br><span class="line">    context, x  = inputs</span><br><span class="line"></span><br><span class="line">    context = self.encoder(context)  # (batch_size, context_len, d_model)</span><br><span class="line"></span><br><span class="line">    x = self.decoder(x, context)  # (batch_size, target_len, d_model)</span><br><span class="line"></span><br><span class="line">    # Final linear layer output.</span><br><span class="line">    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">      # Drop the keras mask, so it doesn't scale the losses/metrics.</span><br><span class="line">      # b/250038731</span><br><span class="line">      del logits._keras_mask</span><br><span class="line">    except AttributeError:</span><br><span class="line">      pass</span><br><span class="line"></span><br><span class="line">    # Return the final output and the attention weights.</span><br><span class="line">    return logits</span><br></pre></td></tr></table></figure>

<p>define the hyperparameters, instantiate the model and test it out:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_layers = 4</span><br><span class="line">d_model = 128</span><br><span class="line">dff = 512</span><br><span class="line">num_heads = 8</span><br><span class="line">dropout_rate = 0.1</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  instantiate the model</span></span><br><span class="line">transformer = Transformer(</span><br><span class="line">    num_layers=num_layers,</span><br><span class="line">    d_model=d_model,</span><br><span class="line">    num_heads=num_heads,</span><br><span class="line">    dff=dff,</span><br><span class="line">    input_vocab_size=tokenizers.pt.get_vocab_size().numpy(),</span><br><span class="line">    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),</span><br><span class="line">    dropout_rate=dropout_rate)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># test</span></span><br><span class="line">output = transformer((pt, en))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(en.shape)</span><br><span class="line"><span class="built_in">print</span>(pt.shape)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># result</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(64, 53)</span></span><br><span class="line"><span class="string">(64, 75)</span></span><br><span class="line"><span class="string">(64, 53, 7010)</span></span><br><span class="line"><span class="string">"""</span> </span><br></pre></td></tr></table></figure>

<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><h3 id="Set-up-the-optimiser"><a href="#Set-up-the-optimiser" class="headerlink" title="Set up the optimiser"></a><strong>Set up the optimiser</strong></h3><p>Use the Adam optimiser with a custom learning rate scheduler according to the formula in the original Transformer <a target="_blank" rel="noopener" href="https://www.google.com/url?q=https://arxiv.org/abs/1706.03762">paper</a>.</p>
<p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.916ex;" xmlns="http://www.w3.org/2000/svg" width="51.031ex" height="2.916ex" role="img" focusable="false" viewBox="0 -883.9 22555.6 1288.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(298,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(749,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(1278,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(1639,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mo" transform="translate(2382.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msubsup" transform="translate(3438.6,0)"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="TeXAtom" transform="translate(553,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(778,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" transform="translate(500,0)"></path><path data-c="35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z" transform="translate(778,0)"></path></g></g><g data-mml-node="TeXAtom" transform="translate(553,-315.7) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(878,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(1363,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(1883,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(2349,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g></g><g data-mml-node="mo" transform="translate(6135.5,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mo" transform="translate(6635.7,0)"><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(833,0)"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1111,0)"></path></g><g data-mml-node="mrow" transform="translate(8469.4,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="28" d="M152 251Q152 646 388 850H416Q422 844 422 841Q422 837 403 816T357 753T302 649T255 482T236 250Q236 124 255 19T301 -147T356 -251T403 -315T422 -340Q422 -343 416 -349H388Q359 -325 332 -296T271 -213T212 -97T170 56T152 251Z"></path></g><g data-mml-node="mi" transform="translate(458,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(927,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(1288,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="msubsup" transform="translate(1754,0)"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="TeXAtom" transform="translate(536,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(778,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" transform="translate(500,0)"></path><path data-c="35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z" transform="translate(778,0)"></path></g></g><g data-mml-node="TeXAtom" transform="translate(536,-247) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(600,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1172,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(3793.8,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(4238.5,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(4707.5,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(5068.5,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="msub" transform="translate(5534.5,0)"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="TeXAtom" transform="translate(536,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(600,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1172,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(7792.3,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mi" transform="translate(8292.5,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(9008.5,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(9537.5,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(9988.5,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(10866.5,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="msubsup" transform="translate(11438.5,0)"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="TeXAtom" transform="translate(536,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(778,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" transform="translate(500,0)"></path><path data-c="35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z" transform="translate(778,0)"></path></g></g><g data-mml-node="TeXAtom" transform="translate(536,-267.6) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(469,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(830,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(1296,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(1799,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g></g></g><g data-mml-node="mo" transform="translate(13628.2,0) translate(0 -0.5)"><path data-c="29" d="M305 251Q305 -145 69 -349H56Q43 -349 39 -347T35 -338Q37 -333 60 -307T108 -239T160 -136T204 27T221 250T204 473T160 636T108 740T60 807T35 839Q35 850 50 850H56H69Q197 743 256 566Q305 425 305 251Z"></path></g></g></g></g></svg></mjx-container></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):</span><br><span class="line">  def __init__(self, d_model, warmup_steps=4000):</span><br><span class="line">    super().__init__()</span><br><span class="line"></span><br><span class="line">    self.d_model = d_model</span><br><span class="line">    self.d_model = tf.cast(self.d_model, tf.float32)</span><br><span class="line"></span><br><span class="line">    self.warmup_steps = warmup_steps</span><br><span class="line"></span><br><span class="line">  def __call__(self, step):</span><br><span class="line">    step = tf.cast(step, dtype=tf.float32)</span><br><span class="line">    arg1 = tf.math.rsqrt(step)</span><br><span class="line">    arg2 = step * (self.warmup_steps ** -1.5)</span><br><span class="line"></span><br><span class="line">    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)</span><br></pre></td></tr></table></figure>

<h3 id="Set-up-the-loss-and-metrics"><a href="#Set-up-the-loss-and-metrics" class="headerlink" title="Set up the loss and metrics"></a><strong>Set up the loss and metrics</strong></h3><p>We will use the cross-entropy loss function (<code>tf.keras.losses.SparseCategoricalCrossentropy</code>)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">def masked_loss(label, pred):</span><br><span class="line">  mask = label != 0</span><br><span class="line">  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(</span><br><span class="line">    from_logits=True, reduction='none')</span><br><span class="line">  loss = loss_object(label, pred)</span><br><span class="line"></span><br><span class="line">  mask = tf.cast(mask, dtype=loss.dtype)</span><br><span class="line">  loss *= mask</span><br><span class="line"></span><br><span class="line">  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)</span><br><span class="line">  return loss</span><br><span class="line"></span><br><span class="line">def masked_accuracy(label, pred):</span><br><span class="line">  pred = tf.argmax(pred, axis=2)</span><br><span class="line">  label = tf.cast(label, pred.dtype)</span><br><span class="line">  match = label == pred</span><br><span class="line"></span><br><span class="line">  mask = label != 0</span><br><span class="line"></span><br><span class="line">  match = match &amp; mask</span><br><span class="line"></span><br><span class="line">  match = tf.cast(match, dtype=tf.float32)</span><br><span class="line">  mask = tf.cast(mask, dtype=tf.float32)</span><br><span class="line">  return tf.reduce_sum(match)/tf.reduce_sum(mask)</span><br></pre></td></tr></table></figure>

<h3 id="Training-the-model"><a href="#Training-the-model" class="headerlink" title="Training the model"></a>Training the model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compile and fit</span></span><br><span class="line">transformer.<span class="built_in">compile</span>(</span><br><span class="line">    loss=masked_loss,</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    metrics=[masked_accuracy])</span><br><span class="line">    </span><br><span class="line">transformer.fit(train_batches,</span><br><span class="line">                epochs=<span class="number">20</span>,</span><br><span class="line">                validation_data=val_batches)    </span><br></pre></td></tr></table></figure>

<h2 id="Run-inference"><a href="#Run-inference" class="headerlink" title="Run inference"></a><strong>Run inference</strong></h2><p>Define the <code>Translator</code> class by subclassing <code>tf.Module</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Translator</span>(tf.Module):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, tokenizers, transformer</span>):</span><br><span class="line">    self.tokenizers = tokenizers</span><br><span class="line">    self.transformer = transformer</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, sentence, max_length=MAX_TOKENS</span>):</span><br><span class="line">    <span class="comment"># The input sentence is Portuguese, hence adding the `[START]` and `[END]` tokens.</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">isinstance</span>(sentence, tf.Tensor)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(sentence.shape) == <span class="number">0</span>:</span><br><span class="line">      sentence = sentence[tf.newaxis]</span><br><span class="line"></span><br><span class="line">    sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()</span><br><span class="line"></span><br><span class="line">    encoder_input = sentence</span><br><span class="line"></span><br><span class="line">    <span class="comment"># As the output language is English, initialize the output with the</span></span><br><span class="line">    <span class="comment"># English `[START]` token.</span></span><br><span class="line">    start_end = self.tokenizers.en.tokenize([<span class="string">''</span>])[<span class="number">0</span>]</span><br><span class="line">    start = start_end[<span class="number">0</span>][tf.newaxis]</span><br><span class="line">    end = start_end[<span class="number">1</span>][tf.newaxis]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># `tf.TensorArray` is required here (instead of a Python list), so that the</span></span><br><span class="line">    <span class="comment"># dynamic-loop can be traced by `tf.function`.</span></span><br><span class="line">    output_array = tf.TensorArray(dtype=tf.int64, size=<span class="number">0</span>, dynamic_size=<span class="literal">True</span>)</span><br><span class="line">    output_array = output_array.write(<span class="number">0</span>, start)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tf.<span class="built_in">range</span>(max_length):</span><br><span class="line">      output = tf.transpose(output_array.stack())</span><br><span class="line">      predictions = self.transformer([encoder_input, output], training=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Select the last token from the `seq_len` dimension.</span></span><br><span class="line">      predictions = predictions[:, -<span class="number">1</span>:, :]  <span class="comment"># Shape `(batch_size, 1, vocab_size)`.</span></span><br><span class="line"></span><br><span class="line">      predicted_id = tf.argmax(predictions, axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Concatenate the `predicted_id` to the output which is given to the</span></span><br><span class="line">      <span class="comment"># decoder as its input.</span></span><br><span class="line">      output_array = output_array.write(i+<span class="number">1</span>, predicted_id[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> predicted_id == end:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    output = tf.transpose(output_array.stack())</span><br><span class="line">    <span class="comment"># The output shape is `(1, tokens)`.</span></span><br><span class="line">    text = tokenizers.en.detokenize(output)[<span class="number">0</span>]  <span class="comment"># Shape: `()`.</span></span><br><span class="line"></span><br><span class="line">    tokens = tokenizers.en.lookup(output)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># `tf.function` prevents us from using the attention_weights that were</span></span><br><span class="line">    <span class="comment"># calculated on the last iteration of the loop.</span></span><br><span class="line">    <span class="comment"># So, recalculate them outside the loop.</span></span><br><span class="line">    self.transformer([encoder_input, output[:,:-<span class="number">1</span>]], training=<span class="literal">False</span>)</span><br><span class="line">    attention_weights = self.transformer.decoder.last_attn_scores</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> text, tokens, attention_weights</span><br></pre></td></tr></table></figure>

<p>Create an instance of this <code>Translator</code> class, and try it out a few times:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">translator = Translator(tokenizers, transformer)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">print_translation</span>(<span class="params">sentence, tokens, ground_truth</span>):</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">f'<span class="subst">{<span class="string">"Input:"</span>:15s}</span>: <span class="subst">{sentence}</span>'</span>)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">f'<span class="subst">{<span class="string">"Prediction"</span>:15s}</span>: <span class="subst">{tokens.numpy().decode(<span class="string">"utf-8"</span>)}</span>'</span>)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">f'<span class="subst">{<span class="string">"Ground truth"</span>:15s}</span>: <span class="subst">{ground_truth}</span>'</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># example one</span></span><br><span class="line">sentence = <span class="string">'este é um problema que temos que resolver.'</span></span><br><span class="line">ground_truth = <span class="string">'this is a problem we have to solve .'</span></span><br><span class="line"></span><br><span class="line">translated_text, translated_tokens, attention_weights = translator(</span><br><span class="line">    tf.constant(sentence))</span><br><span class="line">print_translation(sentence, translated_text, ground_truth)</span><br><span class="line"></span><br><span class="line"><span class="comment">#result</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Input:        : este é um problema que temos que resolver.</span></span><br><span class="line"><span class="string">Prediction    : this is a problem we have to solve .</span></span><br><span class="line"><span class="string">Ground truth  : this is a problem we have to solve .</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># example two</span></span><br><span class="line">sentence = <span class="string">'os meus vizinhos ouviram sobre esta ideia.'</span></span><br><span class="line">ground_truth = <span class="string">'and my neighboring homes heard about this idea .'</span></span><br><span class="line"></span><br><span class="line">translated_text, translated_tokens, attention_weights = translator(</span><br><span class="line">    tf.constant(sentence))</span><br><span class="line">print_translation(sentence, translated_text, ground_truth)</span><br><span class="line"></span><br><span class="line"><span class="comment">#result</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Input:        : os meus vizinhos ouviram sobre esta ideia .</span></span><br><span class="line"><span class="string">Prediction    : and my neighboring homes heard about this idea .</span></span><br><span class="line"><span class="string">Ground truth  : and my neighboring homes heard about this idea .</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># example three</span></span><br><span class="line">sentence = <span class="string">'vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.'</span></span><br><span class="line">ground_truth = <span class="string">"so i'll just share with you some stories very quickly of some magical things that have happened."</span></span><br><span class="line"></span><br><span class="line">translated_text, translated_tokens, attention_weights = translator(</span><br><span class="line">    tf.constant(sentence))</span><br><span class="line">print_translation(sentence, translated_text, ground_truth)</span><br><span class="line"></span><br><span class="line"><span class="comment">#result</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Input:        : vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram .</span></span><br><span class="line"><span class="string">Prediction    : so i'll just share with you some stories very quickly of some magical things that have happened .</span></span><br><span class="line"><span class="string">Ground truth  : so i'll just share with you some stories very quickly of some magical things that have happened .</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h2 id="Export-the-model"><a href="#Export-the-model" class="headerlink" title="Export the model"></a>Export the model</h2><p>Create a class called <code>ExportTranslator</code> by subclassing the <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/Module"><code>tf.Module</code></a> subclass with a <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/function"><code>tf.function</code></a> on the <code>__call__</code> method:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class ExportTranslator(tf.Module):</span><br><span class="line">  def __init__(self, translator):</span><br><span class="line">    self.translator = translator</span><br><span class="line"></span><br><span class="line">  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])</span><br><span class="line">  def __call__(self, sentence):</span><br><span class="line">    (result,</span><br><span class="line">     tokens,</span><br><span class="line">     attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)</span><br><span class="line"></span><br><span class="line">    return result</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">translator = ExportTranslator(translator)</span><br><span class="line">tf.saved_model.save(translator, export_dir=<span class="string">'translator'</span>)</span><br><span class="line">reloaded = tf.saved_model.load(<span class="string">'translator'</span>)</span><br><span class="line"><span class="built_in">print</span>(reloaded(tf.constant(<span class="string">'este é o primeiro livro que eu fiz.'</span>)).numpy().decode(<span class="string">"utf-8"</span>))</span><br><span class="line"><span class="comment"># result: this is the first book I made.</span></span><br></pre></td></tr></table></figure>

<h2 id="GitHub"><a href="#GitHub" class="headerlink" title="GitHub"></a>GitHub</h2><p><a target="_blank" rel="noopener" href="https://github.com/PaddyZz/neural_machine_translation">https://github.com/PaddyZz/neural_machine_translation</a></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>We have finised:</p>
<p>• Enviroment and Dependencies set up<br>• Data Handling (Datasets, Tokenizer, Data Pipeline)<br>• Define the components (encoder,decoder, attention layers and etc)<br>• Train the Model<br>• Run the inference<br>• Export the model</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">paper</a></p>
<p><a target="_blank" rel="noopener" href="https://www.tensorflow.org/datasets/catalog/ted_hrlr_translate#ted_hrlr_translatept_to_en">ted_hrlr_translate</a></p>
<p><a target="_blank" rel="noopener" href="https://www.tensorflow.org/text/tutorials/nmt_with_attention">Neural machine learning with attention</a></p>
<p><a target="_blank" rel="noopener" href="https://www.tensorflow.org/text/tutorials/transformer">Neural machine translation with a Transformer and Keras</a></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Neural machine translation</p><p><a class="link-muted" href="http://paddyzz.github.io/projects/neural_machine_translation/">http://paddyzz.github.io/projects/neural_machine_translation/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Paddy</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>31-05-2024</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>24-10-2024</p></div></div><div class="level-item is-narrow"><div><h6>Categories</h6><a class="link-muted is-uppercase is-size-7 article_license_category" href="/projects/">projects</a></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons article_license_logo" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons article_license_logo" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons article_license_logo" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1.5px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 mb-4" style="line-height:21px;height:0"><i class="fas fa-tags has-text-grey"></i><span> </span><a class="link-muted  tag-uppercase" rel="tag" href="/tags/DL/">DL<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/NLP/">NLP<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/ML/">ML<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/PROJECTS/">PROJECTS<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/Python/">Python<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/Seq-to-Seq/">Seq-to-Seq<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/Tensorflow/">Tensorflow<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/Keras/">Keras<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/transformer/">transformer<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/tokenizer/">tokenizer<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/self-attention/">self-attention<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/translation/">translation</a></div></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=665ef533f75dab0019ade953&amp;&amp;product=inline-share-buttons" defer></script></article></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/curriculum/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">curriculum</span></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://paddyzz.github.io/projects/neural_machine_translation/';
            this.page.identifier = '/projects/neural_machine_translation/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'paddys-disqus' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div> </div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/images/headshot.jpg" alt="Jiahe(Paddy) ZHAO"></figure><p class="title is-size-4 is-block" style="line-height:inherit;margin-bottom:0.5rem;">Jiahe(Paddy) ZHAO</p><p class="is-block" style="font-style:Italic;font-size:0.8rem;">Artificial Intelligence</p><p class="is-block" style="font-style:Italic;font-size:0.8rem;">Machine Learning</p><p class="is-block" style="font-style:Italic;font-size:0.8rem;">Deep Learning</p><p class="is-block" style="font-style:Italic;font-size:0.8rem;">Computer Science</p><p class="is-block" style="font-style:Italic;font-size:0.8rem;">Information Technology</p><p class="is-size-6 is-flex justify-content-center" style="margin-top:0.5rem;"><i class="fas fa-map-marker-alt mr-1"></i><span>Hebei, CHINA</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading widget-profile-article-fontsize-style title-font-style">Posts</p><a href="/archives"><p class="title widget-profile-number-style">14</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading widget-profile-article-fontsize-style title-font-style">Categories</p><a href="/categories"><p class="title widget-profile-number-style">6</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading widget-profile-article-fontsize-style title-font-style">Tags</p><a href="/tags"><p class="title widget-profile-number-style">62</p></a></div></div></nav><div class="level"><a class="level-item fake-button-style is-rounded widget-profile-follow-button-style title-font-style" href="mailto:jiahe.zhao@uq.net.au?body=Hello%2C%0A%0AIt%20would%20be%20greatly%20appreciated%20if%20you%20could%20send%20me%20an%20email%20using%20your%20personal%20email%20client%20rather%20than%20the%20current%20HTML%20popup%20one%2C%20as%20I%20may%20not%20receive%20your%20message.%0A%0AThank%20you." target="_blank" rel="noopener"> <i class="fa fa-envelope" style="font-size:16.75px;margin-right:0.6rem;"></i>Email Me</a><span>  </span><a class="level-item fake-button-style is-rounded widget-profile-follow-button-style title-font-style" href="https://www.linkedin.com/in/jiahe-paddy-zhao-213b24300" target="_blank" rel="noopener"> <i class="fa-brands fa-linkedin color-linkedin" style="font-size:16.75px;margin-right:0.6rem;"></i>LinkedIn</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless widget-profile-logo-style" target="_blank" rel="noopener" title="Github" href="https://github.com/paddyzz"><i class="fa-brands fa-github color-github"></i></a><a class="level-item button is-transparent is-marginless widget-profile-logo-style" target="_blank" rel="noopener" title="Kaggle" href="https://www.kaggle.com/paddyzhao/"><i class="fa-brands fa-kaggle color-kaggle"></i></a><a class="level-item button is-transparent is-marginless widget-profile-logo-style" target="_blank" rel="noopener" title="Facebook" href="https://www.facebook.com/profile.php?id=100087961621533"><i class="fa-brands fa-facebook-f color-facebook"></i></a><a class="level-item button is-transparent is-marginless widget-profile-logo-style" target="_blank" rel="noopener" title="X" href="https://x.com/paddyzhao150568"><i class="fa-brands fa-x-twitter color-X"></i></a><a class="level-item button is-transparent is-marginless widget-profile-logo-style" target="_blank" rel="noopener" title="Discord" href="https://discord.com/users/1001308889062576139"><i class="fa-brands fa-discord color-discord"></i></a></div></div></div><div class="card widget is-sticky" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Introduction"><span class="level-left"><span class="level-item">1</span><span class="level-item">Introduction</span></span></a></li><li><a class="level is-mobile" href="#Set-up"><span class="level-left"><span class="level-item">2</span><span class="level-item">Set up</span></span></a></li><li><a class="level is-mobile" href="#Data-handling"><span class="level-left"><span class="level-item">3</span><span class="level-item">Data handling</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Download-the-dataset"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">Download the dataset</span></span></a></li><li><a class="level is-mobile" href="#Set-up-the-tokenizer"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">Set up the tokenizer</span></span></a></li><li><a class="level is-mobile" href="#Set-up-a-data-pipeline-with-tf-data"><span class="level-left"><span class="level-item">3.3</span><span class="level-item">Set up a data pipeline with tf.data</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Define-the-components"><span class="level-left"><span class="level-item">4</span><span class="level-item">Define the components</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#The-embedding-and-positional-encoding-layer"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">The embedding and positional encoding layer</span></span></a></li><li><a class="level is-mobile" href="#Add-and-normalise"><span class="level-left"><span class="level-item">4.2</span><span class="level-item">Add and normalise</span></span></a></li><li><a class="level is-mobile" href="#The-base-attention-layer"><span class="level-left"><span class="level-item">4.3</span><span class="level-item">The base attention layer</span></span></a></li><li><a class="level is-mobile" href="#The-cross-attention-layer"><span class="level-left"><span class="level-item">4.4</span><span class="level-item">The cross attention layer</span></span></a></li><li><a class="level is-mobile" href="#The-global-self-attention-layer"><span class="level-left"><span class="level-item">4.5</span><span class="level-item">The global self-attention layer</span></span></a></li><li><a class="level is-mobile" href="#The-causal-self-attention-layer"><span class="level-left"><span class="level-item">4.6</span><span class="level-item">The causal self-attention layer</span></span></a></li><li><a class="level is-mobile" href="#The-feed-forward-network"><span class="level-left"><span class="level-item">4.7</span><span class="level-item">The feed forward network</span></span></a></li><li><a class="level is-mobile" href="#The-encoder-layer"><span class="level-left"><span class="level-item">4.8</span><span class="level-item">The encoder layer</span></span></a></li><li><a class="level is-mobile" href="#The-encoder"><span class="level-left"><span class="level-item">4.9</span><span class="level-item">The encoder</span></span></a></li><li><a class="level is-mobile" href="#The-decoder-layer"><span class="level-left"><span class="level-item">4.10</span><span class="level-item">The decoder layer</span></span></a></li><li><a class="level is-mobile" href="#The-decoder"><span class="level-left"><span class="level-item">4.11</span><span class="level-item">The decoder</span></span></a></li><li><a class="level is-mobile" href="#The-Transformer"><span class="level-left"><span class="level-item">4.12</span><span class="level-item">The Transformer</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Training"><span class="level-left"><span class="level-item">5</span><span class="level-item">Training</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Set-up-the-optimiser"><span class="level-left"><span class="level-item">5.1</span><span class="level-item">Set up the optimiser</span></span></a></li><li><a class="level is-mobile" href="#Set-up-the-loss-and-metrics"><span class="level-left"><span class="level-item">5.2</span><span class="level-item">Set up the loss and metrics</span></span></a></li><li><a class="level is-mobile" href="#Training-the-model"><span class="level-left"><span class="level-item">5.3</span><span class="level-item">Training the model</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Run-inference"><span class="level-left"><span class="level-item">6</span><span class="level-item">Run inference</span></span></a></li><li><a class="level is-mobile" href="#Export-the-model"><span class="level-left"><span class="level-item">7</span><span class="level-item">Export the model</span></span></a></li><li><a class="level is-mobile" href="#GitHub"><span class="level-left"><span class="level-item">8</span><span class="level-item">GitHub</span></span></a></li><li><a class="level is-mobile" href="#Conclusion"><span class="level-left"><span class="level-item">9</span><span class="level-item">Conclusion</span></span></a></li><li><a class="level is-mobile" href="#References"><span class="level-left"><span class="level-item">10</span><span class="level-item">References</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><!--!--><!--!--><div class="column-right-shadow is-hidden-widescreen"></div></div></div></div></section><footer class="footer" style="padding:2rem 0.75rem 2rem;"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/images/favicon.png" alt="Paddy - Paddy&#039;s Log Book" height="28"></a><p class="is-size-7"><span>&copy; 2023-2024 Paddy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a> &amp; <a href="/" target="_blank" rel="noopener">Paddy&#039;s FE Tech</a><br><span id="busuanzi_container_page_uv">Visited by <span id="busuanzi_value_page_uv">88</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent article_license_logo is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent article_license_logo is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent article_license_logo is-large" target="_blank" rel="noopener" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><script src="/js/universe.js"></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script><script type="text/javascript" src="/js/gsap_progressbar.js"></script><script type="text/javascript" src="/js/night.js"></script></body></html>