<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Time Series Forecasting - Paddy - Paddy&#039;s Log Book</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Paddy - Paddy&#039;s Log Book"><meta name="msapplication-TileImage" content="/images/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Paddy - Paddy&#039;s Log Book"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Time series forecasting based on a weather time serires dataset partial machine learning and neural network algorithms for prediction tasks"><meta property="og:type" content="blog"><meta property="og:title" content="Time Series Forecasting"><meta property="og:url" content="http://paddyzz.github.io/projects/time_series_forecasting/"><meta property="og:site_name" content="Paddy - Paddy&#039;s Log Book"><meta property="og:description" content="Time series forecasting based on a weather time serires dataset partial machine learning and neural network algorithms for prediction tasks"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%201.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%202.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%203.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%204.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%205.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%206.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%207.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%208.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%209.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%2010.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%2011.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%2012.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%2013.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%2014.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%2015.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%2016.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%2017.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%2018.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%2019.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%2020.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%2021.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%2022.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%2023.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%2024.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%2025.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%2026.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%2027.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%2028.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%2029.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%2030.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%2031.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%2032.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%2033.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%2034.png"><meta property="og:image" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image%2035.png"><meta property="article:published_time" content="2024-09-30T16:00:00.000Z"><meta property="article:modified_time" content="2024-10-23T16:00:00.000Z"><meta property="article:author" content="Paddy"><meta property="article:tag" content="DL"><meta property="article:tag" content="ML"><meta property="article:tag" content="CNN"><meta property="article:tag" content="ModelMetricsFunc"><meta property="article:tag" content="PROJECTS"><meta property="article:tag" content="Python"><meta property="article:tag" content="prediction"><meta property="article:tag" content="TSF"><meta property="article:tag" content="RNN"><meta property="article:tag" content="LSTM"><meta property="article:tag" content="MLflow"><meta property="article:tag" content="ModelEval"><meta property="article:tag" content="Visualistion"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://paddyzz.github.io/images/assets/time_series_forecasting/image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://paddyzz.github.io/projects/time_series_forecasting/"},"headline":"Time Series Forecasting","image":["http://paddyzz.github.io/images/assets/time_series_forecasting/image.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%201.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%202.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%203.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%204.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%205.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%206.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%207.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%208.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%209.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%2010.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%2011.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%2012.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%2013.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%2014.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%2015.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%2016.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%2017.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%2018.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%2019.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%2020.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%2021.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%2022.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%2023.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%2024.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%2025.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%2026.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%2027.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%2028.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%2029.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%2030.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%2031.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%2032.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%2033.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%2034.png","http://paddyzz.github.io/images/assets/time_series_forecasting/image%2035.png"],"datePublished":"2024-09-30T16:00:00.000Z","dateModified":"2024-10-23T16:00:00.000Z","author":{"@type":"Person","name":"Paddy"},"publisher":{"@type":"Organization","name":"Paddy - Paddy's Log Book","logo":{"@type":"ImageObject","url":"http://paddyzz.github.io/images/favicon.png"}},"description":"Time series forecasting based on a weather time serires dataset partial machine learning and neural network algorithms for prediction tasks"}</script><link rel="canonical" href="http://paddyzz.github.io/projects/time_series_forecasting/"><link rel="icon" href="/images/favicon.png"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.2/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><meta name="follow.it-verification-code" content="SdbKP9l6XayZRBYCuvDS"><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-3-column"><script type="text/javascript" src="/js/svg-inject.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/gsap@3.3.4/dist/gsap.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/gsap@3.3.4/dist/ScrollTrigger.js"></script><progress max="100" value="0"></progress><canvas id="universe"></canvas><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/images/favicon.png" alt="Paddy - Paddy&#039;s Log Book" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item navbar-item-home" href="/">PADDY&#039;S LOG BOOK</a><a class="navbar-item" href="/curriculum/">CURRICULUM</a><a class="navbar-item" href="/certificates/">CERTIFICATES</a><a class="navbar-item" href="/blogs/">BLOGS</a><a class="navbar-item" href="/projects/">PROJECTS</a><a class="navbar-item" href="/life/">LIFE</a><a class="navbar-item" href="/archives/">ARCHIVES</a><a class="navbar-item" href="/categories/">CATEGORIES</a><a class="navbar-item" href="/tags/">TAGS</a><a class="navbar-item" href="/faqs/">FAQS</a></div><div class="navbar-end"><a class="navbar-item navbar-item-logo" id="star-nav" title="Nox Tenebratio!" style="opacity:0;display:none;userSelect:none;" href="javascript:;"><i class="fa fa-star-and-crescent" id="star-icon" style="font-size:17.75px"></i></a><a class="navbar-item navbar-item-logo night" id="night-nav" title="Nox!" href="javascript:;"><i class="fas fa-moon" id="night-icon" style="font-size:16.75px"></i></a><a class="navbar-item navbar-item-logo" id="night-nav" title="Email" href="mailto:jiahe.zhao@uq.net.au?body=Hello%2C%0A%0AIt%20would%20be%20greatly%20appreciated%20if%20you%20could%20send%20me%20an%20email%20using%20your%20personal%20email%20client%20rather%20than%20the%20current%20HTML%20popup%20one%2C%20as%20I%20may%20not%20receive%20your%20message.%0A%0AThank%20you."><i class="fas fa-envelope" style="font-size:16.75px"></i></a><a class="navbar-item navbar-item-logo" title="Gitter" target="_blank" rel="noopener" href="https://app.gitter.im/#/room/#Paddy/Community:gitter.im"><i class="fab fa-gitter" style="font-size:16.75px"></i></a><a class="navbar-item navbar-item-logo" title="Element" target="_blank" rel="noopener" href="https://app.element.io/#/room/#Paddy/Community:gitter.im"><svg version="1.0" id="svg-element" xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 225 225"><g fill="#4a4a4a"><path d="M91 1c-11 3-13 19-3 24l11 2c32 3 56 27 59 59 0 9 2 12 6 15 7 4 15 2 19-5l1-13c-1-11-3-19-9-31A87 87 0 0 0 91 1zM73 43a90 90 0 0 0-72 91c3 11 19 13 24 3l2-11c3-32 27-56 59-59 9 0 12-2 15-6 4-7 2-15-5-19H73zM208 82c-7 2-9 6-10 17-3 32-27 56-59 59-9 0-12 2-15 6-4 7-1 15 5 19l13 1c12-1 20-3 33-9a86 86 0 0 0 49-84c-1-4-6-9-8-9h-8zM47 124c-5 3-6 7-6 18 1 12 3 20 10 33 13 28 39 46 71 49 10 1 14 0 18-3 6-6 4-17-3-21l-11-2c-32-3-56-27-59-59l-2-10c-2-4-7-7-11-7l-7 2z"></path></g></svg></a><a class="navbar-item navbar-item-logo" title="GitHub" target="_blank" rel="noopener" href="https://github.com/paddyzz"><i class="fab fa-github" style="font-size:16.75px"></i></a><a class="navbar-item navbar-item-logo" title="Linkedin" target="_blank" rel="noopener" href="https://www.linkedin.com/in/jiahe-paddy-zhao-213b24300"><i class="fab fa-linkedin" style="font-size:19.75px"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item navbar-item-logo search" title="Search For It !" href="javascript:;"><i class="fa-brands fa-searchengin" style="font-size:19.75px"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-3 is-size-4-mobile title-font-style">Time Series Forecasting</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item author-capitalize"> Paddy </span><span class="level-item"><i class="far fa-calendar-alt"></i>&nbsp;<time dateTime="2024-09-30T16:00:00.000Z" title="01/10/2024, 12:00:00 am">01-10-2024</time></span><span class="level-item"><i class="far fa-calendar-check"></i>&nbsp;<time dateTime="2024-10-23T16:00:00.000Z" title="24/10/2024, 12:00:00 am">24-10-2024</time></span><span class="level-item"><i class="far fa-folder-open has-text-grey"></i><span> </span><a class="link-muted" href="/projects/">projects</a></span><span class="level-item"> <i class="far fa-clock"></i> <span> </span> an hour read<span> </span>( About 7060 words)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv">0</span>&nbsp;visits</span></div></div><article class="message is-info" style="margin-top:1.5rem;font-style:oblique"><div class="message-body">Time series forecasting based on a weather time serires dataset partial machine learning and neural network algorithms for prediction tasks</div></article><div class="content card-content-font-style" style="margin-top:1.5rem;margin-bottom:1rem"><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Weather forecasting has always been a critical aspect of our daily lives, influencing various sectors such as agriculture, transportation, and disaster management. Accurate predictions of weather parameters, particularly temperature and humidity, play a vital role in planning and decision-making processes. As climate change continues to introduce variability and uncertainty in weather patterns, the need for reliable forecasting methods becomes increasingly important.</p>
<p>In recent years, advancements in machine learning and data analytics have opened new avenues for improving  prediction models. Traditional statistical methods, while useful, often fall short in capturing complex non-linear relationships within the data. By leveraging time series analysis and modern computational techniques, we can enhance the accuracy of forecasts and provide more timely insights into atmospheric conditions.</p>
<p>This project focuses on developing a time series forecasting model for predicting temperature, humidity levels and other factors based on historical weather data from the dataset. We aim to explore various machine learning algorithms, including regression models, neural networks like CNN and RNN etc, to identify the most effective approach for our prediction task. </p>
<h2 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook</span><br><span class="line">Python 3.10.2</span><br><span class="line">IPython==7.34.0</span><br><span class="line">matplotlib==3.7.1</span><br><span class="line">numpy==1.26.4</span><br><span class="line">pandas==2.1.4</span><br><span class="line">seaborn==0.13.1</span><br><span class="line">tensorflow==2.17.0</span><br></pre></td></tr></table></figure>

<h2 id="Exploratory-Data-Analysis"><a href="#Exploratory-Data-Analysis" class="headerlink" title="Exploratory Data Analysis"></a><strong>Exploratory Data Analysis</strong></h2><h3 id="Set-up"><a href="#Set-up" class="headerlink" title="Set up"></a>Set up</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import datetime</span><br><span class="line"></span><br><span class="line">import IPython</span><br><span class="line">import IPython.display</span><br><span class="line">import matplotlib as mpl</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import seaborn as sns</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>We will use a weather dataset containing 14 features such as air temperature, atmospheric pressure, and humidity to make predictions hourly. These were collected every 10 minutes, beginning in 2003.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">zip_path = tf.keras.utils.get_file(</span><br><span class="line">    origin=&#x27;https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip&#x27;,</span><br><span class="line">    fname=&#x27;jena_climate_2009_2016.csv.zip&#x27;,</span><br><span class="line">    extract=True)</span><br><span class="line">csv_path, _ = os.path.splitext(zip_path)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(csv_path)</span><br><span class="line"># Slice [start:stop:step], starting from index 5 take every 6th record.</span><br><span class="line">df = df[5::6]</span><br><span class="line"></span><br><span class="line">date_time = pd.to_datetime(df.pop(&#x27;Date Time&#x27;), format=&#x27;%d.%m.%Y %H:%M:%S&#x27;)</span><br></pre></td></tr></table></figure>

<p>use df.head() to take a glance</p>
<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image.png" >
</figcaption>

<h3 id="Inspect-and-clean-up"><a href="#Inspect-and-clean-up" class="headerlink" title="Inspect and clean up"></a>Inspect and clean up</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.describe().transpose()</span><br></pre></td></tr></table></figure>

<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 1.png" >
</figcaption>

<p>One thing that should stand out is the <code>min</code> value of the wind velocity (<code>wv (m/s)</code>) and the maximum value (<code>max. wv (m/s)</code>) columns. This <code>-9999</code> is likely erroneous.</p>
<p>There’s a separate wind direction column, so the velocity should be greater than zero (<code>&gt;=0</code>). Replace it with zeros:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">wv = df[&#x27;wv (m/s)&#x27;]</span><br><span class="line">bad_wv = wv == -9999.0</span><br><span class="line">wv[bad_wv] = 0.0</span><br><span class="line"></span><br><span class="line">max_wv = df[&#x27;max. wv (m/s)&#x27;]</span><br><span class="line">bad_max_wv = max_wv == -9999.0</span><br><span class="line">max_wv[bad_max_wv] = 0.0</span><br><span class="line"></span><br><span class="line"># The above inplace edits are reflected in the DataFrame.</span><br><span class="line">df[&#x27;wv (m/s)&#x27;].min()</span><br></pre></td></tr></table></figure>

<h3 id="Feature-engineering"><a href="#Feature-engineering" class="headerlink" title="Feature engineering"></a>Feature engineering</h3><p>The last column of the data, <code>wd (deg)</code>—gives the wind direction in units of degrees. Angles do not make good model inputs: 360° and 0° should be close to each other and wrap around smoothly. Direction shouldn’t matter if the wind is not blowing.</p>
<p>Right now the distribution of wind data looks like this:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.hist2d(df[&#x27;wd (deg)&#x27;], df[&#x27;wv (m/s)&#x27;], bins=(50, 50), vmax=400)</span><br><span class="line">plt.colorbar()</span><br><span class="line">plt.xlabel(&#x27;Wind Direction [deg]&#x27;)</span><br><span class="line">plt.ylabel(&#x27;Wind Velocity [m/s]&#x27;)</span><br></pre></td></tr></table></figure>

<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 2.png" >
</figcaption>

<p>But this will be easier for the model to interpret if converting the wind direction and velocity columns to a wind <strong>vector</strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">wv = df.pop(&#x27;wv (m/s)&#x27;)</span><br><span class="line">max_wv = df.pop(&#x27;max. wv (m/s)&#x27;)</span><br><span class="line"></span><br><span class="line"># Convert to radians.</span><br><span class="line">wd_rad = df.pop(&#x27;wd (deg)&#x27;)*np.pi / 180</span><br><span class="line"></span><br><span class="line"># Calculate the wind x and y components.</span><br><span class="line">df[&#x27;Wx&#x27;] = wv*np.cos(wd_rad)</span><br><span class="line">df[&#x27;Wy&#x27;] = wv*np.sin(wd_rad)</span><br><span class="line"></span><br><span class="line"># Calculate the max wind x and y components.</span><br><span class="line">df[&#x27;max Wx&#x27;] = max_wv*np.cos(wd_rad)</span><br><span class="line">df[&#x27;max Wy&#x27;] = max_wv*np.sin(wd_rad)</span><br></pre></td></tr></table></figure>

<p>The distribution of wind vectors is much simpler for the model to correctly interpret:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.hist2d(df[&#x27;Wx&#x27;], df[&#x27;Wy&#x27;], bins=(50, 50), vmax=400)</span><br><span class="line">plt.colorbar()</span><br><span class="line">plt.xlabel(&#x27;Wind X [m/s]&#x27;)</span><br><span class="line">plt.ylabel(&#x27;Wind Y [m/s]&#x27;)</span><br><span class="line">ax = plt.gca()</span><br><span class="line">ax.axis(&#x27;tight&#x27;)</span><br></pre></td></tr></table></figure>

<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 3.png" >
</figcaption>

<p>Similarly, the <code>Date Time</code> column is very useful, but not in this string form. Start by converting it to seconds:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">timestamp_s = date_time.map(pd.Timestamp.timestamp)</span><br></pre></td></tr></table></figure>

<p>the time in seconds is not a useful model input. Being weather data, it has clear daily and yearly periodicity. And getting usable signals by using sine and cosine transforms to clear “Time of day” and “Time of year” signals:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">day = 24*60*60</span><br><span class="line">year = (365.2425)*day</span><br><span class="line"></span><br><span class="line">df[&#x27;Day sin&#x27;] = np.sin(timestamp_s * (2 * np.pi / day))</span><br><span class="line">df[&#x27;Day cos&#x27;] = np.cos(timestamp_s * (2 * np.pi / day))</span><br><span class="line">df[&#x27;Year sin&#x27;] = np.sin(timestamp_s * (2 * np.pi / year))</span><br><span class="line">df[&#x27;Year cos&#x27;] = np.cos(timestamp_s * (2 * np.pi / year))</span><br><span class="line">plt.plot(np.array(df[&#x27;Day sin&#x27;])[:25])</span><br><span class="line">plt.plot(np.array(df[&#x27;Day cos&#x27;])[:25])</span><br><span class="line">plt.xlabel(&#x27;Time [h]&#x27;)</span><br><span class="line">plt.title(&#x27;Time of day signal&#x27;)</span><br></pre></td></tr></table></figure>

<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 4.png" >
</figcaption>

<h3 id="Spilt-the-data"><a href="#Spilt-the-data" class="headerlink" title="Spilt the data"></a>Spilt the data</h3><p>We will be using a <code>(70%, 20%, 10%)</code> split for the training, validation, and test sets. And the data is <strong>not</strong> being randomly shuffled before splitting. This is for two reasons I think:</p>
<ol>
<li>It ensures that chopping the data into windows of consecutive samples is still possible.</li>
<li>It ensures that the validation&#x2F;test results are more realistic, being evaluated on the data collected after the model was trained.</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">column_indices = &#123;name: i for i, name in enumerate(df.columns)&#125;</span><br><span class="line"></span><br><span class="line">n = len(df)</span><br><span class="line">train_df = df[0:int(n*0.7)]</span><br><span class="line">val_df = df[int(n*0.7):int(n*0.9)]</span><br><span class="line">test_df = df[int(n*0.9):]</span><br><span class="line"></span><br><span class="line">num_features = df.shape[1]</span><br></pre></td></tr></table></figure>

<h3 id="Normalise-the-data"><a href="#Normalise-the-data" class="headerlink" title="Normalise the data"></a>Normalise the data</h3><p> Normalisation is a common way of doing this scaling: subtract the mean and divide by the standard deviation of each feature.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">train_mean = train_df.mean()</span><br><span class="line">train_std = train_df.std()</span><br><span class="line"></span><br><span class="line">train_df = (train_df - train_mean) / train_std</span><br><span class="line">val_df = (val_df - train_mean) / train_std</span><br><span class="line">test_df = (test_df - train_mean) / train_std</span><br></pre></td></tr></table></figure>

<p>Now, peek at the distribution of the features. Some features do have long tails, but there are no obvious errors like the <code>-9999</code> wind velocity value.</p>
<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 5.png" >
</figcaption>
## Data windowing

<p>The main features of the input windows are:</p>
<ul>
<li>The width (number of time steps) of the input and label windows.</li>
<li>The time offset between them.</li>
<li>Which features are used as inputs, labels, or both.</li>
</ul>
<p>This section focuses on implementing the data windowing so that it can be reused for all of those models.</p>
<p>Here are two examples about data windowing:</p>
<p>1.For example, to make a single prediction 24 hours into the future, given 24 hours of history, we might define a window like this:</p>
<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 6.png" >
</figcaption>

<p>2.A model that makes a prediction one hour into the future, given six hours of history, would need a window like this:</p>
<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 7.png" >
</figcaption>

<p><code>WindowGenerator</code> class. This class can:</p>
<ol>
<li>Handle the indexes and offsets as shown in the diagrams above.</li>
<li>Split windows of features into <code>(features, labels)</code> pairs.</li>
<li>Plot the content of the resulting windows.</li>
<li>Efficiently generate batches of these windows from the training, evaluation, and test data, using <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset"><code>tf.data.Dataset</code></a>s.</li>
</ol>
<h3 id="Indexes-and-offsets"><a href="#Indexes-and-offsets" class="headerlink" title="Indexes and offsets"></a>Indexes and offsets</h3><p>Start by creating the <code>WindowGenerator</code> class. The <code>__init__</code> method includes all the necessary logic for the input and label indices.</p>
<p>It also takes the training, evaluation, and test DataFrames as input. These will be converted to <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset"><code>tf.data.Dataset</code></a>s of windows later.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">class WindowGenerator():</span><br><span class="line">  def __init__(self, input_width, label_width, shift,</span><br><span class="line">               train_df=train_df, val_df=val_df, test_df=test_df,</span><br><span class="line">               label_columns=None):</span><br><span class="line">    # Store the raw data.</span><br><span class="line">    self.train_df = train_df</span><br><span class="line">    self.val_df = val_df</span><br><span class="line">    self.test_df = test_df</span><br><span class="line"></span><br><span class="line">    # Work out the label column indices.</span><br><span class="line">    self.label_columns = label_columns</span><br><span class="line">    if label_columns is not None:</span><br><span class="line">      self.label_columns_indices = &#123;name: i for i, name in</span><br><span class="line">                                    enumerate(label_columns)&#125;</span><br><span class="line">    self.column_indices = &#123;name: i for i, name in</span><br><span class="line">                           enumerate(train_df.columns)&#125;</span><br><span class="line"></span><br><span class="line">    # Work out the window parameters.</span><br><span class="line">    self.input_width = input_width</span><br><span class="line">    self.label_width = label_width</span><br><span class="line">    self.shift = shift</span><br><span class="line"></span><br><span class="line">    self.total_window_size = input_width + shift</span><br><span class="line"></span><br><span class="line">    self.input_slice = slice(0, input_width)</span><br><span class="line">    self.input_indices = np.arange(self.total_window_size)[self.input_slice]</span><br><span class="line"></span><br><span class="line">    self.label_start = self.total_window_size - self.label_width</span><br><span class="line">    self.labels_slice = slice(self.label_start, None)</span><br><span class="line">    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]</span><br><span class="line"></span><br><span class="line">  def __repr__(self):</span><br><span class="line">    return &#x27;\n&#x27;.join([</span><br><span class="line">        f&#x27;Total window size: &#123;self.total_window_size&#125;&#x27;,</span><br><span class="line">        f&#x27;Input indices: &#123;self.input_indices&#125;&#x27;,</span><br><span class="line">        f&#x27;Label indices: &#123;self.label_indices&#125;&#x27;,</span><br><span class="line">        f&#x27;Label column name(s): &#123;self.label_columns&#125;&#x27;])</span><br></pre></td></tr></table></figure>

<p>Here is code to create the 2 windows shown in the diagrams at the start of this section:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">w1 = WindowGenerator(input_width=24, label_width=1, shift=24,</span><br><span class="line">                     label_columns=[&#x27;T (degC)&#x27;])</span><br><span class="line">w1</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#w1</span><br><span class="line">Total window size: 48</span><br><span class="line">Input indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]</span><br><span class="line">Label indices: [47]</span><br><span class="line">Label column name(s): [&#x27;T (degC)&#x27;]</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">w2 = WindowGenerator(input_width=6, label_width=1, shift=1,</span><br><span class="line">                     label_columns=[&#x27;T (degC)&#x27;])</span><br><span class="line">w2</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#w2</span><br><span class="line">Total window size: 7</span><br><span class="line">Input indices: [0 1 2 3 4 5]</span><br><span class="line">Label indices: [6]</span><br><span class="line">Label column name(s): [&#x27;T (degC)&#x27;]</span><br></pre></td></tr></table></figure>

<h3 id="Spilt-window"><a href="#Spilt-window" class="headerlink" title="Spilt window"></a>Spilt window</h3><p>Given a list of consecutive inputs, the <code>split_window</code> method will convert them to a window of inputs and a window of labels.</p>
<p>The example <code>w2</code> you define earlier will be split like this:</p>
<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 8.png" >
</figcaption>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def split_window(self, features):</span><br><span class="line">  inputs = features[:, self.input_slice, :]</span><br><span class="line">  labels = features[:, self.labels_slice, :]</span><br><span class="line">  if self.label_columns is not None:</span><br><span class="line">    labels = tf.stack(</span><br><span class="line">        [labels[:, :, self.column_indices[name]] for name in self.label_columns],</span><br><span class="line">        axis=-1)</span><br><span class="line"></span><br><span class="line">  # Slicing doesn&#x27;t preserve static shape information, so set the shapes</span><br><span class="line">  # manually. This way the `tf.data.Datasets` are easier to inspect.</span><br><span class="line">  inputs.set_shape([None, self.input_width, None])</span><br><span class="line">  labels.set_shape([None, self.label_width, None])</span><br><span class="line"></span><br><span class="line">  return inputs, labels</span><br><span class="line"></span><br><span class="line">WindowGenerator.split_window = split_window</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>Test it out:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Stack three slices, the length of the total window.</span><br><span class="line">example_window = tf.stack([np.array(train_df[:w2.total_window_size]),</span><br><span class="line">                           np.array(train_df[100:100+w2.total_window_size]),</span><br><span class="line">                           np.array(train_df[200:200+w2.total_window_size])])</span><br><span class="line"></span><br><span class="line">example_inputs, example_labels = w2.split_window(example_window)</span><br><span class="line"></span><br><span class="line">print(&#x27;All shapes are: (batch, time, features)&#x27;)</span><br><span class="line">print(f&#x27;Window shape: &#123;example_window.shape&#125;&#x27;)</span><br><span class="line">print(f&#x27;Inputs shape: &#123;example_inputs.shape&#125;&#x27;)</span><br><span class="line">print(f&#x27;Labels shape: &#123;example_labels.shape&#125;&#x27;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">All shapes are: (batch, time, features)</span><br><span class="line">Window shape: (3, 7, 19)</span><br><span class="line">Inputs shape: (3, 6, 19)</span><br><span class="line">Labels shape: (3, 1, 1)</span><br></pre></td></tr></table></figure>

<p>The code above took a batch of three 7-time step windows with 19 features at each time step. It splits them into a batch of 6-time step 19-feature inputs, and a 1-time step 1-feature label. The label only has one feature because the <code>WindowGenerator</code> was initialized with <code>label_columns=[&#39;T (degC)&#39;]</code>.</p>
<h3 id="Plot"><a href="#Plot" class="headerlink" title="Plot"></a>Plot</h3><p>Here is a plot method that allows a simple visualization of the split window:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">def plot(self, model=None, plot_col=&#x27;T (degC)&#x27;, max_subplots=3):</span><br><span class="line">  inputs, labels = self.example</span><br><span class="line">  plt.figure(figsize=(12, 8))</span><br><span class="line">  plot_col_index = self.column_indices[plot_col]</span><br><span class="line">  max_n = min(max_subplots, len(inputs))</span><br><span class="line">  for n in range(max_n):</span><br><span class="line">    plt.subplot(max_n, 1, n+1)</span><br><span class="line">    plt.ylabel(f&#x27;&#123;plot_col&#125; [normed]&#x27;)</span><br><span class="line">    plt.plot(self.input_indices, inputs[n, :, plot_col_index],</span><br><span class="line">             label=&#x27;Inputs&#x27;, marker=&#x27;.&#x27;, zorder=-10)</span><br><span class="line"></span><br><span class="line">    if self.label_columns:</span><br><span class="line">      label_col_index = self.label_columns_indices.get(plot_col, None)</span><br><span class="line">    else:</span><br><span class="line">      label_col_index = plot_col_index</span><br><span class="line"></span><br><span class="line">    if label_col_index is None:</span><br><span class="line">      continue</span><br><span class="line"></span><br><span class="line">    plt.scatter(self.label_indices, labels[n, :, label_col_index],</span><br><span class="line">                edgecolors=&#x27;k&#x27;, label=&#x27;Labels&#x27;, c=&#x27;#2ca02c&#x27;, s=64)</span><br><span class="line">    if model is not None:</span><br><span class="line">      predictions = model(inputs)</span><br><span class="line">      plt.scatter(self.label_indices, predictions[n, :, label_col_index],</span><br><span class="line">                  marker=&#x27;X&#x27;, edgecolors=&#x27;k&#x27;, label=&#x27;Predictions&#x27;,</span><br><span class="line">                  c=&#x27;#ff7f0e&#x27;, s=64)</span><br><span class="line"></span><br><span class="line">    if n == 0:</span><br><span class="line">      plt.legend()</span><br><span class="line"></span><br><span class="line">  plt.xlabel(&#x27;Time [h]&#x27;)</span><br><span class="line"></span><br><span class="line">WindowGenerator.plot = plot</span><br></pre></td></tr></table></figure>

<p>test it out:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w2.plot()</span><br></pre></td></tr></table></figure>

<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 9.png" >
</figcaption>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w2.plot(plot_col=<span class="string">&#x27;p (mbar)&#x27;</span>)</span><br></pre></td></tr></table></figure>

<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 10.png" >
</figcaption>

<h3 id="Create-tf-data-Datasets"><a href="#Create-tf-data-Datasets" class="headerlink" title="Create tf.data.Datasets"></a>Create <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset"><code>tf.data.Dataset</code></a>s</h3><p>Finally, this <code>make_dataset</code> method will take a time series DataFrame and convert it to a <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset"><code>tf.data.Dataset</code></a> of <code>(input_window, label_window)</code> pairs using the <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array"><code>tf.keras.utils.timeseries_dataset_from_array</code></a> function:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def make_dataset(self, data):</span><br><span class="line">  data = np.array(data, dtype=np.float32)</span><br><span class="line">  ds = tf.keras.utils.timeseries_dataset_from_array(</span><br><span class="line">      data=data,</span><br><span class="line">      targets=None,</span><br><span class="line">      sequence_length=self.total_window_size,</span><br><span class="line">      sequence_stride=1,</span><br><span class="line">      shuffle=True,</span><br><span class="line">      batch_size=32,)</span><br><span class="line"></span><br><span class="line">  ds = ds.map(self.split_window)</span><br><span class="line"></span><br><span class="line">  return ds</span><br><span class="line"></span><br><span class="line">WindowGenerator.make_dataset = make_dataset</span><br></pre></td></tr></table></figure>

<p>The <code>WindowGenerator</code> object holds training, validation, and test data. Also,  we add a standard example batch for easy access and plotting:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">@property</span><br><span class="line">def train(self):</span><br><span class="line">  return self.make_dataset(self.train_df)</span><br><span class="line"></span><br><span class="line">@property</span><br><span class="line">def val(self):</span><br><span class="line">  return self.make_dataset(self.val_df)</span><br><span class="line"></span><br><span class="line">@property</span><br><span class="line">def test(self):</span><br><span class="line">  return self.make_dataset(self.test_df)</span><br><span class="line"></span><br><span class="line">@property</span><br><span class="line">def example(self):</span><br><span class="line">  &quot;&quot;&quot;Get and cache an example batch of `inputs, labels` for plotting.&quot;&quot;&quot;</span><br><span class="line">  result = getattr(self, &#x27;_example&#x27;, None)</span><br><span class="line">  if result is None:</span><br><span class="line">    # No example batch was found, so get one from the `.train` dataset</span><br><span class="line">    result = next(iter(self.train))</span><br><span class="line">    # And cache it for next time</span><br><span class="line">    self._example = result</span><br><span class="line">  return result</span><br><span class="line"></span><br><span class="line">WindowGenerator.train = train</span><br><span class="line">WindowGenerator.val = val</span><br><span class="line">WindowGenerator.test = test</span><br><span class="line">WindowGenerator.example = example</span><br></pre></td></tr></table></figure>

<p>Now, the <code>WindowGenerator</code> object gives you access to the <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset"><code>tf.data.Dataset</code></a> objects, so we can easily iterate over the data.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Each element is an (inputs, label) pair.</span><br><span class="line">w2.train.element_spec</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(TensorSpec(shape=(None, 6, 19), dtype=tf.float32, name=None),</span><br><span class="line"> TensorSpec(shape=(None, 1, 1), dtype=tf.float32, name=None))</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for example_inputs, example_labels in w2.train.take(1):</span><br><span class="line">  print(f&#x27;Inputs shape (batch, time, features): &#123;example_inputs.shape&#125;&#x27;)</span><br><span class="line">  print(f&#x27;Labels shape (batch, time, features): &#123;example_labels.shape&#125;&#x27;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Inputs shape (batch, time, features): (32, 6, 19)</span><br><span class="line">Labels shape (batch, time, features): (32, 1, 1)</span><br></pre></td></tr></table></figure>

<h2 id="MLFLOW"><a href="#MLFLOW" class="headerlink" title="MLFLOW"></a>MLFLOW</h2><p>before diving into next section, we will set up MLflow(<strong>open source MLOps platform)</strong> for tracking and logging the parameters and  metrics for the models to be implemented and find out the potential model with best performance lining with the time series data</p>
<h3 id="Install-mlflow-and-pyngrok"><a href="#Install-mlflow-and-pyngrok" class="headerlink" title="Install mlflow and pyngrok"></a>Install mlflow and pyngrok</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install mlflow pyngrok --quiet</span><br></pre></td></tr></table></figure>

<h3 id="Config-pyngrok-port-and-set-up-mlflow-UI"><a href="#Config-pyngrok-port-and-set-up-mlflow-UI" class="headerlink" title="Config pyngrok port and set up mlflow UI"></a>Config pyngrok port and set up mlflow UI</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from pyngrok import ngrok</span><br><span class="line">from getpass import getpass</span><br><span class="line">import mlflow</span><br><span class="line"># Terminate open tunnels if exist</span><br><span class="line">ngrok.kill()</span><br><span class="line"></span><br><span class="line">#sign up a new pyngrok account for the AUTH token if not have </span><br><span class="line">NGROK_AUTH_TOKEN = getpass(&#x27;Your_AUTH_TOKEN:&#x27;)</span><br><span class="line">ngrok.set_auth_token(NGROK_AUTH_TOKEN)</span><br><span class="line"></span><br><span class="line"># Open an HTTPs tunnel on port 5000 for http://localhost:5000</span><br><span class="line">ngrok_tunnel = ngrok.connect(addr=&quot;5000&quot;, proto=&quot;http&quot;, bind_tls=True)</span><br><span class="line">print(&quot;MLflow Tracking UI:&quot;, ngrok_tunnel.public_url)</span><br><span class="line">get_ipython().system_raw(&quot;mlflow ui --port 5000 &amp;&quot;)</span><br><span class="line">mlflow.set_experiment(&quot;mlflow_[weather_tsf]_exp_[v1]&quot;)</span><br></pre></td></tr></table></figure>

<p>access MLFLOW UI to test it out, the MLFLOW Track UI: like <a target="_blank" rel="noopener" href="https://17fe-34-70-201-166.ngrok-free.app/">https://17fe-34-70-201-166.ngrok-free.app/</a></p>
<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 11.png" >
</figcaption>

<h3 id="Config-model-params-and-metrics"><a href="#Config-model-params-and-metrics" class="headerlink" title="Config model params and metrics"></a>Config model params and metrics</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># collect params and metrics for mlflow</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mlflow_metrics_eval_model</span>(<span class="params">model_used, model_type, window_used, model_name, metric_result, metric_type</span>):</span><br><span class="line">  <span class="keyword">from</span> mlflow.models <span class="keyword">import</span> infer_signature</span><br><span class="line">  params = &#123;</span><br><span class="line">      <span class="string">&#x27;model_used&#x27;</span>: model_used,</span><br><span class="line">      <span class="string">&#x27;window_used&#x27;</span>: window_used,</span><br><span class="line">      <span class="string">&#x27;model_name&#x27;</span>: model_name,</span><br><span class="line">      <span class="string">&#x27;max_epochs&#x27;</span>: <span class="number">20</span>,</span><br><span class="line">      <span class="string">&#x27;batch_size&#x27;</span>: <span class="number">32</span>,</span><br><span class="line">      <span class="string">&#x27;loss&#x27;</span>: <span class="string">&#x27;mean_squared_error&#x27;</span>,</span><br><span class="line">      <span class="string">&#x27;optimizer&#x27;</span>: <span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">      <span class="string">&#x27;patience&#x27;</span> : <span class="number">2</span>,</span><br><span class="line">      <span class="string">&#x27;input_width&#x27;</span>: window_used.input_width,</span><br><span class="line">      <span class="string">&#x27;label_width&#x27;</span>: window_used.label_width,</span><br><span class="line">      <span class="string">&#x27;shift&#x27;</span>: window_used.shift,</span><br><span class="line">      <span class="string">&#x27;label_columns&#x27;</span>: [<span class="string">&#x27;T (degC)&#x27;</span>]</span><br><span class="line">  &#125;</span><br><span class="line">  metrics = &#123;&#125;</span><br><span class="line">  metrics_key_mapping = &#123;</span><br><span class="line">            <span class="string">&#x27;mean_absolute_error&#x27;</span>: <span class="string">&#x27;mae&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;mean_absolute_percentage_error&#x27;</span>: <span class="string">&#x27;mape&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;root_mean_squared_error&#x27;</span>: <span class="string">&#x27;rmsr&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;loss&#x27;</span> : <span class="string">&#x27;mse&#x27;</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">for</span> key, value <span class="keyword">in</span> metric_result.items():</span><br><span class="line">    <span class="keyword">if</span> key == <span class="string">&#x27;mean_absolute_percentage_error&#x27;</span> <span class="keyword">and</span> value &gt; <span class="number">1</span>:</span><br><span class="line">      value = value / <span class="number">100</span></span><br><span class="line">    metrics_short_key = metrics_key_mapping.get(key, key)</span><br><span class="line">    metrics_short_key = <span class="string">f&quot;<span class="subst">&#123;metrics_short_key&#125;</span><span class="subst">&#123;metric_type&#125;</span>&quot;</span></span><br><span class="line">    metrics[metrics_short_key] = value</span><br><span class="line">  <span class="keyword">with</span> mlflow.start_run():</span><br><span class="line">      mlflow.log_params(params)</span><br><span class="line">      mlflow.log_metrics(metrics)</span><br><span class="line">      mlflow.set_tag(model_type, model_name)</span><br><span class="line">      signature = infer_signature(window_used.example[<span class="number">0</span>].numpy(), model_used(window_used.example[<span class="number">0</span>]).numpy())</span><br><span class="line">      model_info = mlflow.sklearn.log_model(</span><br><span class="line">            sk_model=model_used,</span><br><span class="line">            artifact_path=<span class="string">f&quot;models/<span class="subst">&#123;model_type&#125;</span>/<span class="subst">&#123;model_name&#125;</span>&quot;</span>,</span><br><span class="line">            signature=signature,</span><br><span class="line">            input_example=window_used.example[<span class="number">0</span>].numpy(),</span><br><span class="line">            registered_model_name=model_name,</span><br><span class="line">            conda_env=&#123;</span><br><span class="line">            <span class="string">&#x27;name&#x27;</span>: <span class="string">f&quot;<span class="subst">&#123;model_type&#125;</span>_<span class="subst">&#123;model_name&#125;</span>&quot;</span>,</span><br><span class="line">            <span class="string">&#x27;dependencies&#x27;</span>: [</span><br><span class="line">                <span class="string">&#x27;python=3.10.3&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;IPython==7.34.0&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;matplotlib==3.7.1&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;numpy==1.26.4&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;pandas==2.1.4&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;seaborn==0.13.1&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;tensorflow==2.17.0&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;mlflow&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;pyngrok&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;scikit-learn&#x27;</span></span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<h3 id="Config-mlfow-plot-function-for-plotting-model-metrics"><a href="#Config-mlfow-plot-function-for-plotting-model-metrics" class="headerlink" title="Config mlfow plot function for plotting model metrics"></a>Config mlfow plot function for plotting model metrics</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plot mlflow metrics for models</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_mlflow_metrics</span>(<span class="params">model_type</span>):</span><br><span class="line">    runs = mlflow.search_runs()</span><br><span class="line"></span><br><span class="line">    metrics_list = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> index, row <span class="keyword">in</span> runs.iterrows():</span><br><span class="line">        run_id = row[<span class="string">&#x27;run_id&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        metrics = mlflow.get_run(run_id).data.metrics</span><br><span class="line"></span><br><span class="line">        tags = mlflow.get_run(run_id).data.tags</span><br><span class="line">        tag_value = tags.get(model_type, run_id)</span><br><span class="line"></span><br><span class="line">        metrics_list.append(&#123;<span class="string">&#x27;tag&#x27;</span>: tag_value, **metrics&#125;)</span><br><span class="line"></span><br><span class="line">    metrics_df = pd.DataFrame(metrics_list)</span><br><span class="line"></span><br><span class="line">    metrics_df_melted = metrics_df.melt(id_vars=<span class="string">&#x27;tag&#x27;</span>, var_name=<span class="string">&#x27;Metric&#x27;</span>, value_name=<span class="string">&#x27;Value&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    plt.figure(figsize=(<span class="number">12</span>, <span class="number">6</span>))</span><br><span class="line">    ax = sns.barplot(data=metrics_df_melted, x=<span class="string">&#x27;Metric&#x27;</span>, y=<span class="string">&#x27;Value&#x27;</span>, hue=<span class="string">&#x27;tag&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    plt.title(<span class="string">f&#x27;Metrics with Val_df and Test_df for <span class="subst">&#123;model_type&#125;</span> Models&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Metrics&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Values&#x27;</span>)</span><br><span class="line">    plt.legend(title=model_type)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>

<h2 id="Single-step-models"><a href="#Single-step-models" class="headerlink" title="Single step models"></a>Single step models</h2><p>This blog will introduce two sorts of models, one for single step models with predicting one hour and the other for multi step models with predicting one day.</p>
<p>single step models predicts a single feature’s value—1 time step (one hour) into the future based only on the current conditions.</p>
<p>So, start by building models to predict the <code>T (degC)</code> value one hour into the future</p>
<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 12.png" >
</figcaption>

<p>Configure a <code>WindowGenerator</code> object to produce these single-step <code>(input, label)</code> pairs:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">single_step_window = WindowGenerator(</span><br><span class="line">    input_width=1, label_width=1, shift=1,</span><br><span class="line">    label_columns=[&#x27;T (degC)&#x27;])</span><br></pre></td></tr></table></figure>

<p>And we will create 6 different models in time series forecasting to evaluate their performance</p>
<h3 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h3><p>Before building a trainable model it would be good to have a performance baseline as a point for comparison with the later more complicated models.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class Baseline(tf.keras.Model):</span><br><span class="line">  def __init__(self, label_index=None):</span><br><span class="line">    super().__init__()</span><br><span class="line">    self.label_index = label_index</span><br><span class="line"></span><br><span class="line">  def call(self, inputs):</span><br><span class="line">    if self.label_index is None:</span><br><span class="line">      return inputs</span><br><span class="line">    result = inputs[:, :, self.label_index]</span><br><span class="line">    return result[:, :, tf.newaxis]</span><br></pre></td></tr></table></figure>

<p>Instantiate and evaluate this model:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">baseline = Baseline(label_index=column_indices[<span class="string">&#x27;T (degC)&#x27;</span>])</span><br><span class="line"></span><br><span class="line">baseline.<span class="built_in">compile</span>(loss=tf.keras.losses.MeanSquaredError(),</span><br><span class="line">                 metrics=[tf.keras.metrics.MeanAbsoluteError()])</span><br><span class="line"></span><br><span class="line">val_performance = &#123;&#125;</span><br><span class="line">performance = &#123;&#125;</span><br><span class="line">single_step_performance_key = [<span class="string">&#x27;loss&#x27;</span>,<span class="string">&#x27;mean_absolute_error&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">handle_performance</span>(<span class="params">model_used,model_type, model_name, window_used, val_perf, perf, perf_key_list </span>):</span><br><span class="line">  metrics_val = model_used.evaluate(window_used.val, return_dict=<span class="literal">True</span>)</span><br><span class="line">  metrics_test = model_used.evaluate(window_used.test, verbose=<span class="number">0</span>, return_dict=<span class="literal">True</span>)</span><br><span class="line">  mlflow_metrics_eval_model(model_used, model_type, window_used, model_name, metrics_val,<span class="string">&#x27;_val&#x27;</span>)</span><br><span class="line">  mlflow_metrics_eval_model(model_used, model_type, window_used, model_name, metrics_test,<span class="string">&#x27;_test&#x27;</span>)</span><br><span class="line">  val_perf[model_name] = &#123;key: metrics_val[key] <span class="keyword">for</span> key <span class="keyword">in</span> perf_key_list <span class="keyword">if</span> key <span class="keyword">in</span> metrics_val&#125;</span><br><span class="line">  perf[model_name] = &#123;key: metrics_test[key] <span class="keyword">for</span> key <span class="keyword">in</span> perf_key_list <span class="keyword">if</span> key <span class="keyword">in</span> metrics_test &#125;</span><br><span class="line"></span><br><span class="line">handle_performance(baseline,<span class="string">&#x27;single_step&#x27;</span>,<span class="string">&#x27;Baseline&#x27;</span>,single_step_window,val_performance,performance,single_step_performance_key)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1/439 ━━━━━━━━━━━━━━━━━━━━ 2:27 337ms/step - loss: 0.0075 - mean_absolute_error: 0.0657</span><br><span class="line">WARNING: All log messages before absl::InitializeLog() is called are written to STDERR</span><br><span class="line">I0000 00:00:1723775844.435380   80829 service.cc:146] XLA service 0x7eff1c004a90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:</span><br><span class="line">I0000 00:00:1723775844.435411   80829 service.cc:154]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5</span><br><span class="line">I0000 00:00:1723775844.435416   80829 service.cc:154]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5</span><br><span class="line">I0000 00:00:1723775844.435419   80829 service.cc:154]   StreamExecutor device (2): Tesla T4, Compute Capability 7.5</span><br><span class="line">I0000 00:00:1723775844.435422   80829 service.cc:154]   StreamExecutor device (3): Tesla T4, Compute Capability 7.5</span><br><span class="line">I0000 00:00:1723775844.614710   80829 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.</span><br><span class="line">439/439 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - loss: 0.0121 - mean_absolute_error: 0.0769</span><br></pre></td></tr></table></figure>

<p>That printed some performance metrics, but those don’t give you a feeling for how well the model is doing.</p>
<p>The <code>WindowGenerator</code> has a plot method, but the plots won’t be very interesting with only a single sample.</p>
<p>So, create a wider <code>WindowGenerator</code> that generates windows 24 hours of consecutive inputs and labels at a time. The new <code>wide_window</code> variable doesn’t change the way the model operates. The model still makes predictions one hour into the future based on a single input time step. Here, the <code>time</code> axis acts like the <code>batch</code> axis: each prediction is made independently with no interaction between time steps:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wide_window = WindowGenerator(</span><br><span class="line">    input_width=24, label_width=24, shift=1,</span><br><span class="line">    label_columns=[&#x27;T (degC)&#x27;])</span><br></pre></td></tr></table></figure>

<p>This expanded window can be passed directly to the same <code>baseline</code> model without any code changes. This is possible because the inputs and labels have the same number of time steps, and the baseline just forwards the input to the output:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wide_window.plot(baseline)</span><br></pre></td></tr></table></figure>

<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 13.png" >
</figcaption>

<p>In the above plots of three examples the single step model is run over the course of 24 hours. This deserves some explanation:</p>
<ul>
<li>The blue <code>Inputs</code> line shows the input temperature at each time step. The model receives all features, this plot only shows the temperature.</li>
<li>The green <code>Labels</code> dots show the target prediction value. These dots are shown at the prediction time, not the input time. That is why the range of labels is shifted 1 step relative to the inputs.</li>
<li>The orange <code>Predictions</code> crosses are the model’s prediction’s for each output time step. If the model were predicting perfectly the predictions would land directly on the <code>Labels</code>.</li>
</ul>
<h3 id="Linear-model"><a href="#Linear-model" class="headerlink" title="Linear model"></a>Linear model</h3><p>A <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense"><code>tf.keras.layers.Dense</code></a> layer with no <code>activation</code> set is a linear model. The layer only transforms the last axis of the data from <code>(batch, time, inputs)</code> to <code>(batch, time, units)</code>; it is applied independently to every item across the <code>batch</code> and <code>time</code> axes.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">linear = tf.keras.Sequential([</span><br><span class="line">    tf.keras.layers.Dense(units=1)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<p>compile and fit function:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">MAX_EPOCHS = 20</span><br><span class="line"></span><br><span class="line">def compile_and_fit(model, window, patience=2):</span><br><span class="line">  early_stopping = tf.keras.callbacks.EarlyStopping(monitor=&#x27;val_loss&#x27;,</span><br><span class="line">                                                    patience=patience,</span><br><span class="line">                                                    mode=&#x27;min&#x27;)</span><br><span class="line"></span><br><span class="line">  model.compile(loss=tf.keras.losses.MeanSquaredError(),</span><br><span class="line">                optimizer=tf.keras.optimizers.Adam(),</span><br><span class="line">                metrics=[tf.keras.metrics.MeanAbsoluteError()])</span><br><span class="line"></span><br><span class="line">  history = model.fit(window.train, epochs=MAX_EPOCHS,</span><br><span class="line">                      validation_data=window.val,</span><br><span class="line">                      callbacks=[early_stopping])</span><br><span class="line">  return history</span><br></pre></td></tr></table></figure>

<p>Train the model and evaluate its performance:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">history = compile_and_fit(linear, single_step_window)</span><br><span class="line"></span><br><span class="line">handle_performance(linear,&#x27;single_step&#x27;,&#x27;linear&#x27;,single_step_window,val_performance,performance,single_step_performance_key)</span><br></pre></td></tr></table></figure>

<p>Here is the plot of its example predictions on the <code>wide_window</code>, and how in many cases the prediction is clearly better than just returning the input temperature, but in a few cases it’s worse:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wide_window.plot(linear)</span><br></pre></td></tr></table></figure>

<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 14.png" >
</figcaption>

<p>One advantage to linear models is that they’re relatively simple to interpret. We can pull out the layer’s weights and visualise the weight assigned to each input:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.bar(x = range(len(train_df.columns)),</span><br><span class="line">        height=linear.layers[0].kernel[:,0].numpy())</span><br><span class="line">axis = plt.gca()</span><br><span class="line">axis.set_xticks(range(len(train_df.columns)))</span><br><span class="line">_ = axis.set_xticklabels(train_df.columns, rotation=90)</span><br></pre></td></tr></table></figure>

<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 15.png" >
</figcaption>

<p>Sometimes the model doesn’t even place the most weight on the input <code>T (degC)</code>. This is one of the risks of random initialisation.</p>
<h3 id="Dense"><a href="#Dense" class="headerlink" title="Dense"></a>Dense</h3><p>Here’s a model similar to the <code>linear</code> model for checking the performance of deeper, more powerful, single input step models, except it stacks several a few <code>Dense</code> layers between the input and the output:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dense = tf.keras.Sequential([</span><br><span class="line">    tf.keras.layers.Dense(units=64, activation=&#x27;relu&#x27;),</span><br><span class="line">    tf.keras.layers.Dense(units=64, activation=&#x27;relu&#x27;),</span><br><span class="line">    tf.keras.layers.Dense(units=1)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">history = compile_and_fit(dense, single_step_window)</span><br><span class="line"></span><br><span class="line">handle_performance(dense,&#x27;single_step&#x27;,&#x27;Dense&#x27;,single_step_window,val_performance,performance,single_step_performance_key)</span><br></pre></td></tr></table></figure>

<h3 id="Multi-dense-model"><a href="#Multi-dense-model" class="headerlink" title="Multi dense model"></a>Multi dense model</h3><p>A single-time-step model has no context for the current values of its inputs. It can’t see how the input features are changing over time. To address this issue the model needs access to multiple time steps when making predictions:</p>
<p>The <code>baseline</code>, <code>linear</code> and <code>dense</code> models handled each time step independently. Here the model will take multiple time steps as input to produce a single output.</p>
<p>Create a <code>WindowGenerator</code> that will produce batches of three-hour inputs and one-hour labels:</p>
<p>training a <code>dense</code> model on a multiple-input-step window by adding a <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten"><code>tf.keras.layers.Flatten</code></a> as the first layer of the model:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">multi_step_dense = tf.keras.Sequential([</span><br><span class="line">    # Shape: (time, features) =&gt; (time*features)</span><br><span class="line">    tf.keras.layers.Flatten(),</span><br><span class="line">    tf.keras.layers.Dense(units=32, activation=&#x27;relu&#x27;),</span><br><span class="line">    tf.keras.layers.Dense(units=32, activation=&#x27;relu&#x27;),</span><br><span class="line">    tf.keras.layers.Dense(units=1),</span><br><span class="line">    # Add back the time dimension.</span><br><span class="line">    # Shape: (outputs) =&gt; (1, outputs)</span><br><span class="line">    tf.keras.layers.Reshape([1, -1]),</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">history = compile_and_fit(multi_step_dense, conv_window)</span><br><span class="line"></span><br><span class="line">handle_performance(multi_step_dense,&#x27;single_step&#x27;,&#x27;Multi step dense&#x27;,conv_window,val_performance,performance,single_step_performance_key)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(&#x27;Input shape:&#x27;, conv_window.example[0].shape)</span><br><span class="line">print(&#x27;Output shape:&#x27;, multi_step_dense(conv_window.example[0]).shape)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Input shape: (32, 3, 19)</span><br><span class="line">Output shape: (32, 1, 1)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conv_window.plot(multi_step_dense)</span><br></pre></td></tr></table></figure>

<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 16.png" >
</figcaption>

<p>The main down-side of this approach is that the resulting model can only be executed on input windows of exactly this shape.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(&#x27;Input shape:&#x27;, wide_window.example[0].shape)</span><br><span class="line">try:</span><br><span class="line">  print(&#x27;Output shape:&#x27;, multi_step_dense(wide_window.example[0]).shape)</span><br><span class="line">except Exception as e:</span><br><span class="line">  print(f&#x27;\n&#123;type(e).__name__&#125;:&#123;e&#125;&#x27;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Input shape: (32, 24, 19)</span><br><span class="line"></span><br><span class="line">ValueError:Exception encountered when calling Sequential.call().</span><br><span class="line"></span><br><span class="line">Input 0 of layer &quot;dense_4&quot; is incompatible with the layer: expected axis -1 of input shape to have value 57, but received input with shape (32, 456)</span><br><span class="line"></span><br><span class="line">Arguments received by Sequential.call():</span><br><span class="line">  • inputs=tf.Tensor(shape=(32, 24, 19), dtype=float32)</span><br><span class="line">  • training=None</span><br><span class="line">  • mask=None</span><br></pre></td></tr></table></figure>

<p>The convolution models in the next section fix this problem.</p>
<h3 id="Convolution-neural-network"><a href="#Convolution-neural-network" class="headerlink" title="Convolution neural network"></a>Convolution neural network</h3><p>A convolution layer (<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D"><code>tf.keras.layers.Conv1D</code></a>) also takes multiple time steps as input to each prediction.</p>
<p>Below is the <strong>same</strong> model as <code>multi_step_dense</code>, re-written with a convolution.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">conv_model = tf.keras.Sequential([</span><br><span class="line">    tf.keras.layers.Conv1D(filters=32,</span><br><span class="line">                           kernel_size=(CONV_WIDTH,),</span><br><span class="line">                           activation=&#x27;relu&#x27;),</span><br><span class="line">    tf.keras.layers.Dense(units=32, activation=&#x27;relu&#x27;),</span><br><span class="line">    tf.keras.layers.Dense(units=1),</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<p>Compile and fit the model, handle the performance</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">history = compile_and_fit(conv_model, conv_window)</span><br><span class="line"></span><br><span class="line">IPython.display.clear_output()</span><br><span class="line"></span><br><span class="line">handle_performance(conv_model,&#x27;Conv&#x27;,&#x27;Multi step dense&#x27;,conv_window,val_performance,performance,single_step_performance_key)</span><br></pre></td></tr></table></figure>

<p>check out the input and output tensor shape</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;Conv model on `conv_window`&quot;)</span><br><span class="line">print(&#x27;Input shape:&#x27;, conv_window.example[0].shape)</span><br><span class="line">print(&#x27;Output shape:&#x27;, conv_model(conv_window.example[0]).shape)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Conv model on `conv_window`</span><br><span class="line">Input shape: (32, 3, 19)</span><br><span class="line">Output shape: (32, 1, 1)</span><br></pre></td></tr></table></figure>

<p>check out the input and output tensor shape of wide_window</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;Wide window&quot;)</span><br><span class="line">print(&#x27;Input shape:&#x27;, wide_window.example[0].shape)</span><br><span class="line">print(&#x27;Labels shape:&#x27;, wide_window.example[1].shape)</span><br><span class="line">print(&#x27;Output shape:&#x27;, conv_model(wide_window.example[0]).shape)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Wide window</span><br><span class="line">Input shape: (32, 24, 19)</span><br><span class="line">Labels shape: (32, 24, 1)</span><br><span class="line">Output shape: (32, 22, 1)</span><br><span class="line">W0000 00:00:1723775965.411205   80658 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced</span><br><span class="line">W0000 00:00:1723775965.430143   80658 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced</span><br><span class="line">W0000 00:00:1723775965.431321   80658 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced</span><br><span class="line">W0000 00:00:1723775965.432466   80658 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>We could find out that the output is shorter than the input. To make training or plotting work, we need the labels, and prediction to have the same length. So build a <code>WindowGenerator</code> to produce wide windows with a few extra input time steps so the label and prediction lengths match:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">LABEL_WIDTH = 24</span><br><span class="line">INPUT_WIDTH = LABEL_WIDTH + (CONV_WIDTH - 1)</span><br><span class="line">wide_conv_window = WindowGenerator(</span><br><span class="line">    input_width=INPUT_WIDTH,</span><br><span class="line">    label_width=LABEL_WIDTH,</span><br><span class="line">    shift=1,</span><br><span class="line">    label_columns=[&#x27;T (degC)&#x27;])</span><br><span class="line"></span><br><span class="line">wide_conv_window</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Total window size: 27</span><br><span class="line">Input indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23</span><br><span class="line"> 24 25]</span><br><span class="line">Label indices: [ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26]</span><br><span class="line">Label column name(s): [&#x27;T (degC)&#x27;]</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;Wide conv window&quot;)</span><br><span class="line">print(&#x27;Input shape:&#x27;, wide_conv_window.example[0].shape)</span><br><span class="line">print(&#x27;Labels shape:&#x27;, wide_conv_window.example[1].shape)</span><br><span class="line">print(&#x27;Output shape:&#x27;, conv_model(wide_conv_window.example[0]).shape)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Wide conv window</span><br><span class="line">Input shape: (32, 26, 19)</span><br><span class="line">Labels shape: (32, 24, 1)</span><br><span class="line">Output shape: (32, 24, 1)</span><br><span class="line">W0000 00:00:1723775965.630979   80658 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced</span><br><span class="line">W0000 00:00:1723775965.632244   80658 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced</span><br><span class="line">W0000 00:00:1723775965.633402   80658 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>Every prediction here is based on the 3 preceding time steps:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wide_conv_window.plot(conv_model)</span><br></pre></td></tr></table></figure>

<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 17.png" >
</figcaption>

<h3 id="Recurrent-neural-network"><a href="#Recurrent-neural-network" class="headerlink" title="Recurrent neural network"></a><strong>Recurrent neural network</strong></h3><p>A Recurrent Neural Network (RNN) is a type of neural network well-suited to time series data. RNNs process a time series step-by-step, maintaining an internal state from time-step to time-step. And we will use an RNN layer called Long Short-Term Memory (<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM"><code>tf.keras.layers.LSTM</code></a>).</p>
<p>n important constructor argument for all Keras RNN layers, such as <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM"><code>tf.keras.layers.LSTM</code></a>, is the <code>return_sequences</code> argument. This setting can configure the layer in one of two ways:</p>
<ol>
<li>If <code>False</code>, the default, the layer only returns the output of the final time step, giving the model time to warm up its internal state before making a single prediction:</li>
</ol>
<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 18.png" >
</figcaption>

<ol>
<li><p>If <code>True</code>, the layer returns an output for each input. This is useful for:</p>
<ul>
<li>Stacking RNN layers.</li>
<li>Training a model on multiple time steps simultaneously.</li>
</ul>
 <figcaption align="center">
 <img src="../../images/assets/time_series_forecasting/image 19.png" >
 </figcaption></li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">lstm_model = tf.keras.models.Sequential([</span><br><span class="line">    # Shape [batch, time, features] =&gt; [batch, time, lstm_units]</span><br><span class="line">    tf.keras.layers.LSTM(32, return_sequences=True),</span><br><span class="line">    # Shape =&gt; [batch, time, features]</span><br><span class="line">    tf.keras.layers.Dense(units=1)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">history = compile_and_fit(lstm_model, wide_window)</span><br><span class="line"></span><br><span class="line">IPython.display.clear_output()</span><br><span class="line"></span><br><span class="line">handle_performance(lstm_model,&#x27;single_step&#x27;,&#x27;LSTM&#x27;,wide_window,val_performance,performance,single_step_performance_key)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(&#x27;Input shape:&#x27;, wide_window.example[0].shape)</span><br><span class="line">print(&#x27;Output shape:&#x27;, lstm_model(wide_window.example[0]).shape)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Input shape: (32, 24, 19)</span><br><span class="line">Output shape: (32, 24, 1)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wide_window.plot(lstm_model)</span><br></pre></td></tr></table></figure>

<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 20.png" >
</figcaption>

<h3 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a><strong>Performance</strong></h3><p>log in MLFLOW UI and check out the metrics</p>
<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 21.png" >
</figcaption>

<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 22.png" >
</figcaption>

<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 23.png" >
</figcaption>

<p>use MLFLOW plot func to present and analyse the metrics</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_mlflow_metrics(<span class="string">&#x27;single_step&#x27;</span>)</span><br></pre></td></tr></table></figure>

<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 24.png" >
</figcaption>

<p>from the picture and we could find out  the <em><strong>LSTM</strong></em> model metrics are better than other models’ based on the test and validation dataset according to the four different  common used metrics in time series data prediction, which are <code>mean_absolute_error</code>, <code>mean_square_error</code>, <code>mean_absolute_percentage_error</code>, <code>symmetric_mean_absolute_percentage_error</code> and <code>root_mean_square_error</code></p>
<p>Diving into deeper on comparison of the mae metric for the models</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cm = lstm_model.metrics[1]</span><br><span class="line">cm.metrics</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&lt;MeanAbsoluteError name=mean_absolute_error&gt;]</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val_performance</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;Baseline&#x27;: &#123;&#x27;loss&#x27;: 0.012845644727349281,</span><br><span class="line">  &#x27;mean_absolute_error&#x27;: 0.07846628874540329&#125;,</span><br><span class="line"> &#x27;Linear&#x27;: &#123;&#x27;loss&#x27;: 0.008695926517248154,</span><br><span class="line">  &#x27;mean_absolute_error&#x27;: 0.06866316497325897&#125;,</span><br><span class="line"> &#x27;Dense&#x27;: &#123;&#x27;loss&#x27;: 0.006793886888772249,</span><br><span class="line">  &#x27;mean_absolute_error&#x27;: 0.05716359242796898&#125;,</span><br><span class="line"> &#x27;Multi step dense&#x27;: &#123;&#x27;loss&#x27;: 0.007616413291543722,</span><br><span class="line">  &#x27;mean_absolute_error&#x27;: 0.06059327721595764&#125;,</span><br><span class="line"> &#x27;Conv&#x27;: &#123;&#x27;loss&#x27;: 0.006222909316420555,</span><br><span class="line">  &#x27;mean_absolute_error&#x27;: 0.05451442673802376&#125;,</span><br><span class="line"> &#x27;LSTM&#x27;: &#123;&#x27;loss&#x27;: 0.0056776562705636024,</span><br><span class="line">  &#x27;mean_absolute_error&#x27;: 0.05233458802103996&#125; &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(len(performance))</span><br><span class="line">width = 0.3</span><br><span class="line">metric_name = &#x27;mean_absolute_error&#x27;</span><br><span class="line">val_mae = [v[metric_name] for v in val_performance.values()]</span><br><span class="line">test_mae = [v[metric_name] for v in performance.values()]</span><br><span class="line"></span><br><span class="line">plt.ylabel(&#x27;mean_absolute_error [T (degC), normalized]&#x27;)</span><br><span class="line">plt.bar(x - 0.17, val_mae, width, label=&#x27;Validation&#x27;)</span><br><span class="line">plt.bar(x + 0.17, test_mae, width, label=&#x27;Test&#x27;)</span><br><span class="line">plt.xticks(ticks=x, labels=performance.keys(),</span><br><span class="line">           rotation=45)</span><br><span class="line">_ = plt.legend()</span><br></pre></td></tr></table></figure>

<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 25.png" >
</figcaption>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for name, value in performance.items():</span><br><span class="line">  print(f&#x27;&#123;name:12s&#125;: &#123;value[metric_name]:0.4f&#125;&#x27;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Baseline    : 0.0852</span><br><span class="line">Linear      : 0.0663</span><br><span class="line">Dense       : 0.0584</span><br><span class="line">Multi step dense: 0.0633</span><br><span class="line">Conv        : 0.0543</span><br><span class="line">LSTM        : 0.0533</span><br></pre></td></tr></table></figure>

<p>With this dataset typically each of the models does slightly better than the one before it and</p>
<p>from the data, the performance of the model based on <em><strong>LSTM</strong></em> model is better than other’s, which is 0.0533, when predicting single time step.</p>
<h2 id="Multi-step-models"><a href="#Multi-step-models" class="headerlink" title="Multi-step models"></a><strong>Multi-step models</strong></h2><p>This section looks at how to expand these models to make <strong>multiple time step predictions</strong>.</p>
<p>In a multi-step prediction, the model needs to learn to predict a range of future values. Thus, unlike a single step model, where only a single future point is predicted, a multi-step model predicts a sequence of the future values.</p>
<p>There are two rough approaches to this:</p>
<ol>
<li>Single shot predictions where the entire time series is predicted at once.</li>
<li>Autoregressive predictions where the model only makes single step predictions and its output is fed back as its input.</li>
</ol>
<p>For the multi-step model, the training data again consists of hourly samples. However, here, the models will learn to predict 24 hours into the future, given 24 hours of the past.</p>
<p>Here is a <code>Window</code> object that generates these slices from the dataset:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">OUT_STEPS = 24</span><br><span class="line">multi_window = WindowGenerator(input_width=24,</span><br><span class="line">                               label_width=OUT_STEPS,</span><br><span class="line">                               shift=OUT_STEPS)</span><br><span class="line"></span><br><span class="line">multi_window.plot()</span><br><span class="line">multi_window</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Total window size: 48</span><br><span class="line">Input indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]</span><br><span class="line">Label indices: [24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47]</span><br><span class="line">Label column name(s): None</span><br></pre></td></tr></table></figure>

<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 26.png" >
</figcaption>

<h3 id="Baselines"><a href="#Baselines" class="headerlink" title="Baselines"></a>Baselines</h3><p>A simple baseline for this task is to repeat the last input time step for the required number of output time steps:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">class MultiStepLastBaseline(tf.keras.Model):</span><br><span class="line">  def call(self, inputs):</span><br><span class="line">    return tf.tile(inputs[:, -1:, :], [1, OUT_STEPS, 1])</span><br><span class="line"></span><br><span class="line">last_baseline = MultiStepLastBaseline()</span><br><span class="line">last_baseline.compile(loss=tf.keras.losses.MeanSquaredError(),</span><br><span class="line">                      metrics=[tf.keras.metrics.MeanAbsoluteError()])</span><br><span class="line"></span><br><span class="line">multi_val_performance = &#123;&#125;</span><br><span class="line">multi_performance = &#123;&#125;</span><br><span class="line"></span><br><span class="line">multi_step_performance_key = [&#x27;loss&#x27;, &#x27;mean_absolute_error&#x27;]</span><br><span class="line"></span><br><span class="line">handle_performance(last_baseline,&#x27;multi_step&#x27;,&#x27;Last&#x27;,multi_window,multi_val_performance,multi_performance,multi_step_performance_key)</span><br><span class="line"></span><br><span class="line">multi_window.plot(last_baseline)</span><br></pre></td></tr></table></figure>

<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 27.png" >
</figcaption>

<p>Since this task is to predict 24 hours into the future, given 24 hours of the past, another simple approach is to repeat the previous day, assuming tomorrow will be similar:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class RepeatBaseline(tf.keras.Model):</span><br><span class="line">  def call(self, inputs):</span><br><span class="line">    return inputs</span><br><span class="line"></span><br><span class="line">repeat_baseline = RepeatBaseline()</span><br><span class="line">repeat_baseline.compile(loss=tf.keras.losses.MeanSquaredError(),</span><br><span class="line">                        metrics=[tf.keras.metrics.MeanAbsoluteError()])</span><br><span class="line"></span><br><span class="line">handle_performance(repeat_baseline,&#x27;multi_step&#x27;,&#x27;Repeat&#x27;, multi_window, multi_val_performance, multi_performance,multi_step_performance_key)</span><br><span class="line"></span><br><span class="line">multi_window.plot(repeat_baseline)</span><br></pre></td></tr></table></figure>

<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 28.png" >
</figcaption>

<h3 id="Linear"><a href="#Linear" class="headerlink" title="Linear"></a>Linear</h3><p>A simple linear model based on the last input time step does better than either baseline, but is underpowered. The model needs to predict <code>OUTPUT_STEPS</code> time steps, from a single input time step with a linear projection. It can only capture a low-dimensional slice of the behavior, likely based mainly on the time of day and time of year.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">multi_linear_model = tf.keras.Sequential([</span><br><span class="line">    # Take the last time-step.</span><br><span class="line">    # Shape [batch, time, features] =&gt; [batch, 1, features]</span><br><span class="line">    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),</span><br><span class="line">    # Shape =&gt; [batch, 1, out_steps*features]</span><br><span class="line">    tf.keras.layers.Dense(OUT_STEPS*num_features,</span><br><span class="line">                          kernel_initializer=tf.initializers.zeros()),</span><br><span class="line">    # Shape =&gt; [batch, out_steps, features]</span><br><span class="line">    tf.keras.layers.Reshape([OUT_STEPS, num_features])</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">history = compile_and_fit(multi_linear_model, multi_window)</span><br><span class="line"></span><br><span class="line">IPython.display.clear_output()</span><br><span class="line"></span><br><span class="line">handle_performance(multi_linear_model,&#x27;multi_step&#x27;,&#x27;Linear&#x27;,multi_window,multi_val_performance,multi_performance,multi_step_performance_key)</span><br><span class="line"></span><br><span class="line">multi_window.plot(multi_linear_model)</span><br></pre></td></tr></table></figure>

<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 29.png" >
</figcaption>
### Dense

<p>Adding a <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense"><code>tf.keras.layers.Dense</code></a> between the input and output gives the linear model more power</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">multi_dense_model = tf.keras.Sequential([</span><br><span class="line">    # Take the last time step.</span><br><span class="line">    # Shape [batch, time, features] =&gt; [batch, 1, features]</span><br><span class="line">    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),</span><br><span class="line">    # Shape =&gt; [batch, 1, dense_units]</span><br><span class="line">    tf.keras.layers.Dense(512, activation=&#x27;relu&#x27;),</span><br><span class="line">    # Shape =&gt; [batch, out_steps*features]</span><br><span class="line">    tf.keras.layers.Dense(OUT_STEPS*num_features,</span><br><span class="line">                          kernel_initializer=tf.initializers.zeros()),</span><br><span class="line">    # Shape =&gt; [batch, out_steps, features]</span><br><span class="line">    tf.keras.layers.Reshape([OUT_STEPS, num_features])</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">history = compile_and_fit(multi_dense_model, multi_window)</span><br><span class="line"></span><br><span class="line">IPython.display.clear_output()</span><br><span class="line"></span><br><span class="line">handle_performance(multi_dense_model,&#x27;multi_step&#x27;,&#x27;Dense&#x27;,multi_window,multi_val_performance,multi_performance,multi_step_performance_key)</span><br><span class="line"></span><br><span class="line">multi_window.plot(multi_dense_model)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 30.png" >
</figcaption>

<h3 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h3><p>A convolutional model makes predictions based on a fixed-width history, which may lead to better performance than the dense model since it can see how things are changing over time:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">CONV_WIDTH = <span class="number">3</span></span><br><span class="line">multi_conv_model = tf.keras.Sequential([</span><br><span class="line">    <span class="comment"># Shape [batch, time, features] =&gt; [batch, CONV_WIDTH, features]</span></span><br><span class="line">    tf.keras.layers.Lambda(<span class="keyword">lambda</span> x: x[:, -CONV_WIDTH:, :]),</span><br><span class="line">    <span class="comment"># Shape =&gt; [batch, 1, conv_units]</span></span><br><span class="line">    tf.keras.layers.Conv1D(<span class="number">256</span>, activation=<span class="string">&#x27;relu&#x27;</span>, kernel_size=(CONV_WIDTH)),</span><br><span class="line">    <span class="comment"># Shape =&gt; [batch, 1,  out_steps*features]</span></span><br><span class="line">    tf.keras.layers.Dense(OUT_STEPS*num_features,</span><br><span class="line">                          kernel_initializer=tf.initializers.zeros()),</span><br><span class="line">    <span class="comment"># Shape =&gt; [batch, out_steps, features]</span></span><br><span class="line">    tf.keras.layers.Reshape([OUT_STEPS, num_features])</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">history = compile_and_fit(multi_conv_model, multi_window)</span><br><span class="line"></span><br><span class="line">IPython.display.clear_output()</span><br><span class="line"></span><br><span class="line">handle_performance(multi_conv_model,<span class="string">&#x27;multi_step&#x27;</span>,<span class="string">&#x27;Conv&#x27;</span>,multi_window,multi_val_performance,multi_performance,multi_step_performance_key)</span><br><span class="line"></span><br><span class="line">multi_window.plot(multi_conv_model)</span><br></pre></td></tr></table></figure>

<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 31.png" >
</figcaption>

<h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>A recurrent model can learn to use a long history of inputs, if it’s relevant to the predictions the model is making. Here the model will accumulate internal state for 24 hours, before making a single prediction for the next 24 hours. In this single-shot format, the LSTM only needs to produce an output at the last time step, so set <code>return_sequences=False</code> in <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM"><code>tf.keras.layers.LSTM</code></a>.</p>
<p>In this multi-step format, the LSTM only needs to produce an output at the last time step, so set <code>return_sequences=False</code> in <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM"><code>tf.keras.layers.LSTM</code></a>.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">multi_lstm_model = tf.keras.Sequential([</span><br><span class="line">    # Shape [batch, time, features] =&gt; [batch, lstm_units].</span><br><span class="line">    # Adding more `lstm_units` just overfits more quickly.</span><br><span class="line">    tf.keras.layers.LSTM(32, return_sequences=False),</span><br><span class="line">    # Shape =&gt; [batch, out_steps*features].</span><br><span class="line">    tf.keras.layers.Dense(OUT_STEPS*num_features,</span><br><span class="line">                          kernel_initializer=tf.initializers.zeros()),</span><br><span class="line">    # Shape =&gt; [batch, out_steps, features].</span><br><span class="line">    tf.keras.layers.Reshape([OUT_STEPS, num_features])</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">history = compile_and_fit(multi_lstm_model, multi_window)</span><br><span class="line"></span><br><span class="line">IPython.display.clear_output()</span><br><span class="line"></span><br><span class="line">handle_performance(multi_lstm_model,&#x27;multi_step&#x27;,&#x27;LSTM&#x27;,multi_window,multi_val_performance,multi_performance,multi_step_performance_key)</span><br><span class="line"></span><br><span class="line">multi_window.plot(multi_lstm_model)</span><br></pre></td></tr></table></figure>

<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 32.png" >
</figcaption>

<h3 id="Autoregressive-RNN-model"><a href="#Autoregressive-RNN-model" class="headerlink" title="Autoregressive RNN model"></a>Autoregressive RNN model</h3><p>The above models all predict the entire output sequence in a single step.</p>
<p>In some cases it may be helpful for the model to decompose this prediction into individual time steps. Then, each model’s output can be fed back into itself at each step and predictions can be made conditioned on the previous one.</p>
<p>The model will have the same basic form as the single-step LSTM models from earlier: a <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM"><code>tf.keras.layers.LSTM</code></a> layer followed by a <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense"><code>tf.keras.layers.Dense</code></a> layer that converts the <code>LSTM</code> layer’s outputs to model predictions.</p>
<p>A <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM"><code>tf.keras.layers.LSTM</code></a> is a <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell"><code>tf.keras.layers.LSTMCell</code></a> wrapped in the higher level <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN"><code>tf.keras.layers.RNN</code></a> that manages the state and sequence results for you (Check out the <a target="_blank" rel="noopener" href="https://www.tensorflow.org/guide/keras/rnn">Recurrent Neural Networks (RNN) with Keras</a> guide for details).</p>
<p>In this case, the model has to manually manage the inputs for each step, so it uses <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell"><code>tf.keras.layers.LSTMCell</code></a> directly for the lower level, single time step interface.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class FeedBack(tf.keras.Model):</span><br><span class="line">  def __init__(self, units, out_steps):</span><br><span class="line">    super().__init__()</span><br><span class="line">    self.out_steps = out_steps</span><br><span class="line">    self.units = units</span><br><span class="line">    self.lstm_cell = tf.keras.layers.LSTMCell(units)</span><br><span class="line">    # Also wrap the LSTMCell in an RNN to simplify the `warmup` method.</span><br><span class="line">    self.lstm_rnn = tf.keras.layers.RNN(self.lstm_cell, return_state=True)</span><br><span class="line">    self.dense = tf.keras.layers.Dense(num_features)</span><br><span class="line">feedback_model = FeedBack(units=32, out_steps=OUT_STEPS)</span><br></pre></td></tr></table></figure>

<p>The first method this model needs is a <code>warmup</code> method to initialize its internal state based on the inputs. Once trained, this state will capture the relevant parts of the input history. This is equivalent to the single-step <code>LSTM</code> model from earlier</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def warmup(self, inputs):</span><br><span class="line">  # inputs.shape =&gt; (batch, time, features)</span><br><span class="line">  # x.shape =&gt; (batch, lstm_units)</span><br><span class="line">  x, *state = self.lstm_rnn(inputs)</span><br><span class="line"></span><br><span class="line">  # predictions.shape =&gt; (batch, features)</span><br><span class="line">  prediction = self.dense(x)</span><br><span class="line">  return prediction, state</span><br><span class="line"></span><br><span class="line">FeedBack.warmup = warmup</span><br></pre></td></tr></table></figure>

<p>This method returns a single time-step prediction and the internal state of the <code>LSTM</code>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">prediction, state = feedback_model.warmup(multi_window.example[0])</span><br><span class="line">prediction.shape</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TensorShape([32, 19])</span><br></pre></td></tr></table></figure>

<p>With the <code>RNN</code>‘s state, and an initial prediction you can now continue iterating the model feeding the predictions at each step back as the input.</p>
<p>One of the simplest approach for collecting the output predictions could be  using a Python list and a <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/stack"><code>tf.stack</code></a> after the loop.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">def call(self, inputs, training=None):</span><br><span class="line">  # Use a TensorArray to capture dynamically unrolled outputs.</span><br><span class="line">  predictions = []</span><br><span class="line">  # Initialize the LSTM state.</span><br><span class="line">  prediction, state = self.warmup(inputs)</span><br><span class="line"></span><br><span class="line">  # Insert the first prediction.</span><br><span class="line">  predictions.append(prediction)</span><br><span class="line"></span><br><span class="line">  # Run the rest of the prediction steps.</span><br><span class="line">  for n in range(1, self.out_steps):</span><br><span class="line">    # Use the last prediction as input.</span><br><span class="line">    x = prediction</span><br><span class="line">    # Execute one lstm step.</span><br><span class="line">    x, state = self.lstm_cell(x, states=state,</span><br><span class="line">                              training=training)</span><br><span class="line">    # Convert the lstm output to a prediction.</span><br><span class="line">    prediction = self.dense(x)</span><br><span class="line">    # Add the prediction to the output.</span><br><span class="line">    predictions.append(prediction)</span><br><span class="line"></span><br><span class="line">  # predictions.shape =&gt; (time, batch, features)</span><br><span class="line">  predictions = tf.stack(predictions)</span><br><span class="line">  # predictions.shape =&gt; (batch, time, features)</span><br><span class="line">  predictions = tf.transpose(predictions, [1, 0, 2])</span><br><span class="line">  return predictions</span><br><span class="line"></span><br><span class="line">FeedBack.call = call</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history = compile_and_fit(feedback_model, multi_window)</span><br><span class="line"></span><br><span class="line">IPython.display.clear_output()</span><br><span class="line"></span><br><span class="line">handle_performance(feedback_model,&#x27;multi_step&#x27;,&#x27;AR LSTM&#x27;,multi_window,multi_val_performance,multi_performance,multi_step_performance_key)</span><br><span class="line"></span><br><span class="line">multi_window.plot(feedback_model)</span><br></pre></td></tr></table></figure>

<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 33.png" >
</figcaption>

<h3 id="Performance-1"><a href="#Performance-1" class="headerlink" title="Performance"></a>Performance</h3><p><strong>MLFLOW metrics</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_mlflow_metrics(<span class="string">&#x27;multi_step&#x27;</span>)</span><br></pre></td></tr></table></figure>

<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 34.png" >
</figcaption>

<p>LSTM model performs well generally except on <code>mean_abosolute_percentage_error</code>.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">x = np.arange(len(multi_performance))</span><br><span class="line">width = 0.3</span><br><span class="line"></span><br><span class="line">metric_name = &#x27;mean_absolute_error&#x27;</span><br><span class="line">val_mae = [v[metric_name] for v in multi_val_performance.values()]</span><br><span class="line">test_mae = [v[metric_name] for v in multi_performance.values()]</span><br><span class="line"></span><br><span class="line">plt.bar(x - 0.17, val_mae, width, label=&#x27;Validation&#x27;)</span><br><span class="line">plt.bar(x + 0.17, test_mae, width, label=&#x27;Test&#x27;)</span><br><span class="line">plt.xticks(ticks=x, labels=multi_performance.keys(),</span><br><span class="line">           rotation=45)</span><br><span class="line">plt.ylabel(f&#x27;MAE (average over all times and outputs)&#x27;)</span><br><span class="line">_ = plt.legend()</span><br></pre></td></tr></table></figure>

<figcaption align="center">
  <img src="../../images/assets/time_series_forecasting/image 35.png" >
</figcaption>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for name, value in multi_performance.items():</span><br><span class="line">  print(f&#x27;&#123;name:8s&#125;: &#123;value[metric_name]:0.4f&#125;&#x27;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Last    : 0.5157</span><br><span class="line">Repeat  : 0.3774</span><br><span class="line">Linear  : 0.2980</span><br><span class="line">Dense   : 0.2765</span><br><span class="line">Conv    : 0.2732</span><br><span class="line">LSTM    : 0.2767</span><br><span class="line">AR LSTM : 0.2910</span><br></pre></td></tr></table></figure>

<p>The gains achieved going from a dense model to convolutional and recurrent models are only a few percent (if any), and the autoregressive model performed clearly worse. So these more complex approaches may not be worth while on <strong>this</strong> problem, but there was no way to know without trying.</p>
<p>Finally, we will use LSTM model to implement our time series data forecasting. Checking out the GitHub repo below for complete implementing LSTM model with the configs above on the prediction tasks and the blog: <a href="https://paddyzz.github.io/Projects/Config_Kubeflow/">Deploy the model on Kubeflow</a></p>
<h2 id="GitHub"><a href="#GitHub" class="headerlink" title="GitHub"></a>GitHub</h2><p><a target="_blank" rel="noopener" href="https://github.com/PaddyZz/Time_Series_Forecasting">https://github.com/PaddyZz/Time_Series_Forecasting</a></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>We have finished:</p>
<p>• Environment and dependencies set up<br>• Exploratory Data Analysis<br>• Configure MLflow for model evaluation and model metrics visualisation<br>• Compile, fit, train and evaluate the models with single step type<br>• Compile, fit, train and evaluate the models with multi step type<br>• Compare the metrics for choosing best performance model among them</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a target="_blank" rel="noopener" href="https://www.tensorflow.org/tutorials/structured_data/time_series">time series forecasting</a></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Time Series Forecasting</p><p><a class="link-muted" href="http://paddyzz.github.io/projects/time_series_forecasting/">http://paddyzz.github.io/projects/time_series_forecasting/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Paddy</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>01-10-2024</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>24-10-2024</p></div></div><div class="level-item is-narrow"><div><h6>Categories</h6><a class="link-muted is-uppercase is-size-7 article_license_category" href="/projects/">projects</a></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons article_license_logo" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons article_license_logo" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons article_license_logo" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1.5px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 mb-4" style="line-height:21px;height:0"><i class="fas fa-tags has-text-grey"></i><span> </span><a class="link-muted  tag-uppercase" rel="tag" href="/tags/DL/">DL<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/ML/">ML<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/CNN/">CNN<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/ModelMetricsFunc/">ModelMetricsFunc<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/PROJECTS/">PROJECTS<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/Python/">Python<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/prediction/">prediction<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/TSF/">TSF<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/RNN/">RNN<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/LSTM/">LSTM<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/MLflow/">MLflow<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/ModelEval/">ModelEval<span>, </span></a><a class="link-muted  tag-uppercase" rel="tag" href="/tags/Visualistion/">Visualistion</a></div></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=665ef533f75dab0019ade953&amp;&amp;product=inline-share-buttons" defer></script></article></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/projects/Config_Kubeflow/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Configure and deploy models on Kubeflow</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/projects/Config_Sagemaker/"><span class="level-item">Configure and deploy models on Sagemaker</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://paddyzz.github.io/projects/time_series_forecasting/';
            this.page.identifier = '/projects/time_series_forecasting/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'paddys-disqus' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div> </div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://assets-global.website-files.com/653dcbdd718cbd5c51c4dbed/653f5e8549a0fbf7dc295818_IMG_20190205_193746-p-500.jpg" alt="Jiahe(Paddy) ZHAO"></figure><p class="title is-size-4 is-block" style="line-height:inherit;margin-bottom:0.5rem;">Jiahe(Paddy) ZHAO</p><p class="is-block" style="font-style:Italic;font-size:0.8rem;">Artificial Intelligence</p><p class="is-block" style="font-style:Italic;font-size:0.8rem;">Machine Learning</p><p class="is-block" style="font-style:Italic;font-size:0.8rem;">Deep Learning</p><p class="is-block" style="font-style:Italic;font-size:0.8rem;">Computer Science</p><p class="is-block" style="font-style:Italic;font-size:0.8rem;">Information Technology</p><p class="is-size-6 is-flex justify-content-center" style="margin-top:0.5rem;"><i class="fas fa-map-marker-alt mr-1"></i><span>Hebei, CHINA</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading widget-profile-article-fontsize-style title-font-style">Posts</p><a href="/archives"><p class="title widget-profile-number-style">14</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading widget-profile-article-fontsize-style title-font-style">Categories</p><a href="/categories"><p class="title widget-profile-number-style">5</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading widget-profile-article-fontsize-style title-font-style">Tags</p><a href="/tags"><p class="title widget-profile-number-style">61</p></a></div></div></nav><div class="level"><a class="level-item fake-button-style is-rounded widget-profile-follow-button-style title-font-style" href="mailto:jiahe.zhao@uq.net.au?body=Hello%2C%0A%0AIt%20would%20be%20greatly%20appreciated%20if%20you%20could%20send%20me%20an%20email%20using%20your%20personal%20email%20client%20rather%20than%20the%20current%20HTML%20popup%20one%2C%20as%20I%20may%20not%20receive%20your%20message.%0A%0AThank%20you." target="_blank" rel="noopener"> <i class="fa fa-envelope" style="font-size:16.75px;margin-right:0.6rem;"></i>Email Me</a><span>  </span><a class="level-item fake-button-style is-rounded widget-profile-follow-button-style title-font-style" href="https://www.linkedin.com/in/jiahe-paddy-zhao-213b24300" target="_blank" rel="noopener"> <i class="fa-brands fa-linkedin color-linkedin" style="font-size:16.75px;margin-right:0.6rem;"></i>LinkedIn</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless widget-profile-logo-style" target="_blank" rel="noopener" title="Github" href="https://github.com/paddyzz"><i class="fa-brands fa-github color-github"></i></a><a class="level-item button is-transparent is-marginless widget-profile-logo-style" target="_blank" rel="noopener" title="Kaggle" href="https://www.kaggle.com/paddyzhao/"><i class="fa-brands fa-kaggle color-kaggle"></i></a><a class="level-item button is-transparent is-marginless widget-profile-logo-style" target="_blank" rel="noopener" title="Facebook" href="https://www.facebook.com/profile.php?id=100087961621533"><i class="fa-brands fa-facebook-f color-facebook"></i></a><a class="level-item button is-transparent is-marginless widget-profile-logo-style" target="_blank" rel="noopener" title="X" href="https://x.com/paddyzhao150568"><i class="fa-brands fa-x-twitter color-X"></i></a><a class="level-item button is-transparent is-marginless widget-profile-logo-style" target="_blank" rel="noopener" title="Discord" href="https://discord.com/users/1001308889062576139"><i class="fa-brands fa-discord color-discord"></i></a></div></div></div><div class="card widget is-sticky" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Introduction"><span class="level-left"><span class="level-item">1</span><span class="level-item">Introduction</span></span></a></li><li><a class="level is-mobile" href="#Configuration"><span class="level-left"><span class="level-item">2</span><span class="level-item">Configuration</span></span></a></li><li><a class="level is-mobile" href="#Exploratory-Data-Analysis"><span class="level-left"><span class="level-item">3</span><span class="level-item">Exploratory Data Analysis</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Set-up"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">Set up</span></span></a></li><li><a class="level is-mobile" href="#Dataset"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">Dataset</span></span></a></li><li><a class="level is-mobile" href="#Inspect-and-clean-up"><span class="level-left"><span class="level-item">3.3</span><span class="level-item">Inspect and clean up</span></span></a></li><li><a class="level is-mobile" href="#Feature-engineering"><span class="level-left"><span class="level-item">3.4</span><span class="level-item">Feature engineering</span></span></a></li><li><a class="level is-mobile" href="#Spilt-the-data"><span class="level-left"><span class="level-item">3.5</span><span class="level-item">Spilt the data</span></span></a></li><li><a class="level is-mobile" href="#Normalise-the-data"><span class="level-left"><span class="level-item">3.6</span><span class="level-item">Normalise the data</span></span></a></li><li><a class="level is-mobile" href="#Indexes-and-offsets"><span class="level-left"><span class="level-item">3.7</span><span class="level-item">Indexes and offsets</span></span></a></li><li><a class="level is-mobile" href="#Spilt-window"><span class="level-left"><span class="level-item">3.8</span><span class="level-item">Spilt window</span></span></a></li><li><a class="level is-mobile" href="#Plot"><span class="level-left"><span class="level-item">3.9</span><span class="level-item">Plot</span></span></a></li><li><a class="level is-mobile" href="#Create-tf-data-Datasets"><span class="level-left"><span class="level-item">3.10</span><span class="level-item">Create tf.data.Datasets</span></span></a></li></ul></li><li><a class="level is-mobile" href="#MLFLOW"><span class="level-left"><span class="level-item">4</span><span class="level-item">MLFLOW</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Install-mlflow-and-pyngrok"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">Install mlflow and pyngrok</span></span></a></li><li><a class="level is-mobile" href="#Config-pyngrok-port-and-set-up-mlflow-UI"><span class="level-left"><span class="level-item">4.2</span><span class="level-item">Config pyngrok port and set up mlflow UI</span></span></a></li><li><a class="level is-mobile" href="#Config-model-params-and-metrics"><span class="level-left"><span class="level-item">4.3</span><span class="level-item">Config model params and metrics</span></span></a></li><li><a class="level is-mobile" href="#Config-mlfow-plot-function-for-plotting-model-metrics"><span class="level-left"><span class="level-item">4.4</span><span class="level-item">Config mlfow plot function for plotting model metrics</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Single-step-models"><span class="level-left"><span class="level-item">5</span><span class="level-item">Single step models</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Baseline"><span class="level-left"><span class="level-item">5.1</span><span class="level-item">Baseline</span></span></a></li><li><a class="level is-mobile" href="#Linear-model"><span class="level-left"><span class="level-item">5.2</span><span class="level-item">Linear model</span></span></a></li><li><a class="level is-mobile" href="#Dense"><span class="level-left"><span class="level-item">5.3</span><span class="level-item">Dense</span></span></a></li><li><a class="level is-mobile" href="#Multi-dense-model"><span class="level-left"><span class="level-item">5.4</span><span class="level-item">Multi dense model</span></span></a></li><li><a class="level is-mobile" href="#Convolution-neural-network"><span class="level-left"><span class="level-item">5.5</span><span class="level-item">Convolution neural network</span></span></a></li><li><a class="level is-mobile" href="#Recurrent-neural-network"><span class="level-left"><span class="level-item">5.6</span><span class="level-item">Recurrent neural network</span></span></a></li><li><a class="level is-mobile" href="#Performance"><span class="level-left"><span class="level-item">5.7</span><span class="level-item">Performance</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Multi-step-models"><span class="level-left"><span class="level-item">6</span><span class="level-item">Multi-step models</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Baselines"><span class="level-left"><span class="level-item">6.1</span><span class="level-item">Baselines</span></span></a></li><li><a class="level is-mobile" href="#Linear"><span class="level-left"><span class="level-item">6.2</span><span class="level-item">Linear</span></span></a></li><li><a class="level is-mobile" href="#CNN"><span class="level-left"><span class="level-item">6.3</span><span class="level-item">CNN</span></span></a></li><li><a class="level is-mobile" href="#RNN"><span class="level-left"><span class="level-item">6.4</span><span class="level-item">RNN</span></span></a></li><li><a class="level is-mobile" href="#Autoregressive-RNN-model"><span class="level-left"><span class="level-item">6.5</span><span class="level-item">Autoregressive RNN model</span></span></a></li><li><a class="level is-mobile" href="#Performance-1"><span class="level-left"><span class="level-item">6.6</span><span class="level-item">Performance</span></span></a></li></ul></li><li><a class="level is-mobile" href="#GitHub"><span class="level-left"><span class="level-item">7</span><span class="level-item">GitHub</span></span></a></li><li><a class="level is-mobile" href="#Conclusion"><span class="level-left"><span class="level-item">8</span><span class="level-item">Conclusion</span></span></a></li><li><a class="level is-mobile" href="#References"><span class="level-left"><span class="level-item">9</span><span class="level-item">References</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><!--!--><!--!--><div class="column-right-shadow is-hidden-widescreen"></div></div></div></div></section><footer class="footer" style="padding:2rem 0.75rem 2rem;"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/images/favicon.png" alt="Paddy - Paddy&#039;s Log Book" height="28"></a><p class="is-size-7"><span>&copy; 2023-2024 Paddy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a> &amp; <a href="/" target="_blank" rel="noopener">Paddy&#039;s FE Tech</a><br><span id="busuanzi_container_page_uv">Visited by <span id="busuanzi_value_page_uv">88</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent article_license_logo is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent article_license_logo is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent article_license_logo is-large" target="_blank" rel="noopener" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><script src="/js/universe.js"></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script><script type="text/javascript" src="/js/gsap_progressbar.js"></script><script type="text/javascript" src="/js/night.js"></script></body></html>